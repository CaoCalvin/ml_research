{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5e364c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c054d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.3.9)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadr in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pyreadr) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2.0->pyreadr) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2.0->pyreadr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2.0->pyreadr) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2.0->pyreadr) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadr) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (1.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.1.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from xgboost) (2.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from xgboost) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (3.10.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typeguard in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from typeguard) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyQt6 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (6.8.0)\n",
      "Requirement already satisfied: PyQt6-sip<14,>=13.8 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from PyQt6) (13.9.1)\n",
      "Requirement already satisfied: PyQt6-Qt6<6.9.0,>=6.8.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from PyQt6) (6.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smogn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from smogn) (2.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from smogn) (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from smogn) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->smogn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->smogn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->smogn) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from tqdm->smogn) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->smogn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.13.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: imbalanced-learn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (2.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (0.61.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from numba->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (0.61.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from numba) (0.44.0)\n",
      "Requirement already satisfied: numpy<2.2,>=1.24 in c:\\users\\kevin\\dev\\ml_research\\venv\\lib\\site-packages (from numba) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install dill\n",
    "%pip install pyreadr\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy==2.1.0\n",
    "%pip install xgboost\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install typeguard\n",
    "%pip install PyQt6\n",
    "%pip install smogn\n",
    "%pip install seaborn\n",
    "%pip install imbalanced-learn\n",
    "%pip install shap\n",
    "%pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a6cbc7-e77d-47fa-b099-e0f911c53168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library to check function types of imported modules\n",
    "from typeguard import install_import_hook\n",
    "\n",
    "# Data import and export\n",
    "import pyreadr\n",
    "import dill\n",
    "\n",
    "# Data management libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, percentileofscore\n",
    "\n",
    "# Plotting libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Model training and evaluation\n",
    "import shap\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "# Preprocessing\n",
    "from smogn import smoter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# K-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Custom functions for plotting, data operations, and model training\n",
    "with install_import_hook('custom_ml_plots'):\n",
    "    import custom_ml_plots as cmp\n",
    "with install_import_hook('custom_dataset_tools'):\n",
    "    import custom_dataset_tools as cdt\n",
    "with install_import_hook('basic_ml_operations'):\n",
    "    import basic_ml_operations as bmo\n",
    "with install_import_hook('ml_data_objects'):\n",
    "    import ml_data_objects as mdo\n",
    "with install_import_hook('pandas_relational_algebra'):\n",
    "    import pandas_relational_algebra as pra\n",
    "\n",
    "# Global parameters\n",
    "RANDOM_STATE = 42\n",
    "TOP_THRESHOLD_QUANTILE = 0.8  # Values to test: 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "SMOGN_PREPROCESS = True\n",
    "UNDERSAMPLE = True\n",
    "SHAP = False # Set to True to run SHAP analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80ae051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dill session\n",
    "dill.load_session(r\"C:\\Users\\kevin\\dev\\ml_research\\saved_data_and_plots\\thresh_0.8_undersample_False\\project_ipynb_env_RO.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91967ef",
   "metadata": {},
   "source": [
    "## Setup Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shaded_scatter_grids(y_preds_grid: np.ndarray, y_test_grid: np.ndarray, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, pearson_grid: np.ndarray, plot_title: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot predictions vs actuals and colour by pearson coefficient and add best fit\n",
    "    Created: 2024/11/30\n",
    "\n",
    "    Args:\n",
    "        y_preds_grid (np.ndarray): 2D array of predicted values from different models.\n",
    "        y_test_grid (np.ndarray): 2D array of actual values corresponding to the predictions.\n",
    "        axis1_params (mdo.AxisParams): Hyperparameters for the first axis.\n",
    "        axis2_params (mdo.AxisParams): Hyperparameters for the second axis.\n",
    "        pearson_grid (np.ndarray): 2D array of Pearson coefficients for each model.\n",
    "        plot_title (str): Title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The resulting figure object containing the scatter plots.\n",
    "    \"\"\"\n",
    "    # Create a grid of scatter plots with predictions vs actuals\n",
    "    fig, axs = cmp.create_scatter_grid(y_preds_grid, y_test_grid, axis1_params, axis2_params, plot_title)\n",
    "\n",
    "    # Color the scatter plots by Pearson coefficient and add best fit lines and title\n",
    "    cmp.color_spectrum(fig, axs, pearson_grid, label=\"Pearson Coefficient\")\n",
    "    cmp.add_best_fit(axs)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b3abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shaded_roc_grids(y_preds_grid: np.ndarray, y_test_grid: np.ndarray, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, f1_grid: np.ndarray, plot_title: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot predictions vs actuals and colour by f1 score and add best fit\n",
    "    Created: 2024/12/22\n",
    "\n",
    "    Args:\n",
    "        y_preds_grid (np.ndarray): 2D array of predicted probabilities from different models.\n",
    "        y_test_grid (np.ndarray): 2D array of actual binary values corresponding to the predictions.\n",
    "        axis1_params (mdo.AxisParams): Hyperparameters for the first axis.\n",
    "        axis2_params (mdo.AxisParams): Hyperparameters for the second axis.\n",
    "        f1_grid (np.ndarray): 2D array of F1 scores for each model.\n",
    "        plot_title (str): Title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The resulting figure object containing the ROC plots.\n",
    "    \"\"\"\n",
    "    # Create a grid of ROC plots with predictions vs actuals\n",
    "    fig, axs = cmp.create_roc_grid(y_preds_grid, y_test_grid, axis1_params, axis2_params, plot_title)\n",
    "\n",
    "    # Color the ROC plots by F1 score and add best fit lines and title\n",
    "    cmp.color_spectrum(fig, axs, f1_grid, label=\"F1 Score\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13ebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores average metrics for each model for final comparison\n",
    "# See Montesinos-Lopez research paper and README for details on metrics and the different models\n",
    "B_average_metrics = pd.DataFrame(columns=['F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "R_average_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "RO_average_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "# Add existing GBLUP regression model data from Montesinos-Lopez paper\n",
    "B_average_metrics.loc['GBLUP'] =  [0.411, 0.696, 0.577, 0.180]\n",
    "R_average_metrics.loc['GBLUP'] =  [None, 0.215, 0.128, 0.987, 0.164]\n",
    "RO_average_metrics.loc['GBLUP'] = [None, 0.487, 0.711, 0.699, 0.304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740e548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data: pd.DataFrame, title: str, x_ax_label: str = 'Grain Yield (GY)', y_ax_label: str = 'Frequency', vline_value: float = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a stacked histogram of grain yield values for each column in the DataFrame.\n",
    "    Created: 2024/01/12\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing values; can have multiple columns for a stacked histogram\n",
    "        title (str): Title of the plot\n",
    "        x_ax_label (str): Label for the x-axis (default is 'Grain Yield (GY)')\n",
    "        y_ax_label (str): Label for the y-axis (default is 'Frequency')\n",
    "        vline_value (float, optional): Value at which to draw a dotted red vertical line (default is None)\n",
    "    \"\"\"\n",
    "    # Plot stacked histogram for each column in the DataFrame\n",
    "    data.plot.hist(stacked=True, bins=60, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # Add labels for x-axis and y-axis\n",
    "    plt.xlabel(x_ax_label)\n",
    "    plt.ylabel(y_ax_label)\n",
    "    \n",
    "    # Add title to the plot\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Draw a dotted red vertical line if vline_value is specified\n",
    "    if vline_value is not None:\n",
    "        plt.axvline(x=vline_value, color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    # Save the plot as an SVG file in the specified storage directory\n",
    "    plt.savefig(f'{storage_dir}\\\\{title}.svg', format=\"svg\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Close the plot to free up memory\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3df08d",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a24d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numbered_subdir():\n",
    "    \"\"\"\n",
    "    Creates a new subdirectory within the 'saved_data_and_plots' directory, \n",
    "    with a name that is the next available number in sequence, formatted as a \n",
    "    three-digit number (e.g., '001', '002', etc.).\n",
    "    Created: 2024/01/01\n",
    "    Returns:\n",
    "        str: The path to the newly created numbered subdirectory.\n",
    "    \"\"\"\n",
    "    # Define the parent directory where subdirectories will be created\n",
    "    parent_dir = \"saved_data_and_plots\"\n",
    "    \n",
    "    # Create parent directory if it doesn't exist\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    \n",
    "    # List all existing directories within the parent directory\n",
    "    existing_dirs = [d for d in os.listdir(parent_dir) \n",
    "                    if os.path.isdir(os.path.join(parent_dir, d))]\n",
    "    \n",
    "    # Extract numeric values from directory names and find the next available number\n",
    "    existing_nums = [int(d) for d in existing_dirs if d.isdigit()]\n",
    "    next_num = max(existing_nums + [-1]) + 1\n",
    "    \n",
    "    # Create the new numbered directory with the next available number\n",
    "    new_dir = os.path.join(parent_dir, f\"{next_num:03d}\")\n",
    "    os.makedirs(new_dir)\n",
    "    \n",
    "    # Return the path to the newly created directory\n",
    "    return new_dir\n",
    "\n",
    "# Create a new numbered subdirectory and store its path in the variable 'storage_dir'\n",
    "storage_dir = create_numbered_subdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e06ae33-23b5-42a2-a4c1-e063428c27b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>6.119272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>5.905515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>2.160587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>6.456711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688880</th>\n",
       "      <td>3.616688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GY\n",
       "GID                 \n",
       "GID6569128  6.119272\n",
       "GID6569128  5.905515\n",
       "GID6569128  2.160587\n",
       "GID6569128  6.456711\n",
       "GID6688880  3.616688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.517416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.333979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.305651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.301989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.517454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.748336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.394285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               GY\n",
       "count  766.000000\n",
       "mean     5.517416\n",
       "std      0.333979\n",
       "min      4.305651\n",
       "25%      5.301989\n",
       "50%      5.517454\n",
       "75%      5.748336\n",
       "max      6.394285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "eyt1 = pyreadr.read_r('./data/eyt1.RData')\n",
    "\n",
    "# Extract training example labels\n",
    "y = eyt1['Pheno_Disc_Env1'][['GY']]\n",
    "\n",
    "# Set index to gene IDs and sort by index\n",
    "y = y.set_index(eyt1['Pheno_Disc_Env1']['GID'])\n",
    "y = y.sort_index()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(y.head())\n",
    "\n",
    "# Check for missing values\n",
    "cdt.assert_no_bad_values(y)\n",
    "\n",
    "# Each seed was planted in 4 different environments, but we don't care about environmental differences\n",
    "# So we take the average of every group of four rows to reduce the dataset to 1/4 its original size\n",
    "y = cdt.avg_rows(y, 4)\n",
    "\n",
    "# Plot histogram of grain yield values\n",
    "histogram(y, 'GY, Unscaled')\n",
    "\n",
    "# Display summary statistics of the dataset\n",
    "y.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6a9b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939899</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>0.788801</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.157880</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>-0.110899</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.199459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688880</th>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-0.201346</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072171</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.004447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688916</th>\n",
       "      <td>0.025987</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>1.170073</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.196780</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>-0.013459</td>\n",
       "      <td>0.126464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428609</td>\n",
       "      <td>0.423184</td>\n",
       "      <td>0.427788</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>0.163524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688933</th>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>0.402479</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.102718</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079312</td>\n",
       "      <td>-0.087824</td>\n",
       "      <td>-0.089912</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688934</th>\n",
       "      <td>-0.157880</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.996666</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.395843</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>-0.138902</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.026372</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>-0.154557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939917</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>-0.114305</td>\n",
       "      <td>0.062388</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144931</td>\n",
       "      <td>0.144932</td>\n",
       "      <td>0.155032</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>1.112390</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.087428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939919</th>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>-0.141205</td>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.070163</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255040</td>\n",
       "      <td>0.249602</td>\n",
       "      <td>0.262775</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.986131</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.167608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939938</th>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.149919</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719743</td>\n",
       "      <td>0.728707</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>1.118190</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.389739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939941</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>-0.206353</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.084022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465269</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.473004</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>1.070441</td>\n",
       "      <td>0.219306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939945</th>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>1.145606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>766 rows × 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  \\\n",
       "GID6569128    0.788801   -0.006443    0.025987   -0.138795   -0.157880   \n",
       "GID6688880   -0.006443    0.980542    0.064585   -0.168773   -0.081006   \n",
       "GID6688916    0.025987    0.064585    1.170073   -0.021636   -0.031717   \n",
       "GID6688933   -0.138795   -0.168773   -0.021636    0.879004    0.443678   \n",
       "GID6688934   -0.157880   -0.081006   -0.031717    0.443678    0.996666   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "GID6939917    0.004096    0.104630    0.006038   -0.140529   -0.098509   \n",
       "GID6939919    0.091188    0.113878    0.209395   -0.088961   -0.052304   \n",
       "GID6939938    0.074009    0.108757    0.240468   -0.096740   -0.012778   \n",
       "GID6939941    0.032992    0.154718    0.255337   -0.159136   -0.100318   \n",
       "GID6939945    0.199459    0.004447    0.163524   -0.108800   -0.154557   \n",
       "\n",
       "            GID6688949  GID6689407  GID6689482  GID6689550  GID6738288  ...  \\\n",
       "GID6569128    0.096213   -0.110899    0.013069   -0.040445    0.007931  ...   \n",
       "GID6688880    0.078890   -0.201346    0.124671    0.253505    0.013636  ...   \n",
       "GID6688916    0.101532   -0.196780    0.041900   -0.013459    0.126464  ...   \n",
       "GID6688933   -0.080560    0.402479   -0.218803   -0.102718   -0.002303  ...   \n",
       "GID6688934   -0.140766    0.395843   -0.310471   -0.138902    0.088169  ...   \n",
       "...                ...         ...         ...         ...         ...  ...   \n",
       "GID6939917    0.048248   -0.114305    0.062388    0.060255    0.034630  ...   \n",
       "GID6939919    0.139241   -0.141205    0.184798    0.070163    0.015030  ...   \n",
       "GID6939938    0.058980   -0.149919    0.000392    0.063573    0.090066  ...   \n",
       "GID6939941    0.045545   -0.206353    0.194754    0.043889    0.084022  ...   \n",
       "GID6939945    0.152167   -0.163107    0.160584    0.030285    0.010552  ...   \n",
       "\n",
       "            GID6939899  GID6939900  GID6939902  GID6939903  GID6939904  \\\n",
       "GID6569128    0.125612    0.133808    0.137456    0.127674    0.130468   \n",
       "GID6688880    0.072171    0.061650    0.057898    0.079085    0.061086   \n",
       "GID6688916    0.428609    0.423184    0.427788    0.408326    0.426844   \n",
       "GID6688933   -0.079312   -0.087824   -0.089912   -0.067028   -0.084206   \n",
       "GID6688934   -0.016690   -0.017375   -0.026372   -0.014478   -0.016350   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "GID6939917    0.144931    0.144932    0.155032    0.150037    0.161459   \n",
       "GID6939919    0.255040    0.249602    0.262775    0.242206    0.253641   \n",
       "GID6939938    0.719743    0.728707    0.732558    0.708062    0.704652   \n",
       "GID6939941    0.465269    0.461691    0.473004    0.432339    0.435412   \n",
       "GID6939945    0.660434    0.664936    0.645452    0.639670    0.649749   \n",
       "\n",
       "            GID6939917  GID6939919  GID6939938  GID6939941  GID6939945  \n",
       "GID6569128    0.004096    0.091188    0.074009    0.032992    0.199459  \n",
       "GID6688880    0.104630    0.113878    0.108757    0.154718    0.004447  \n",
       "GID6688916    0.006038    0.209395    0.240468    0.255337    0.163524  \n",
       "GID6688933   -0.140529   -0.088961   -0.096740   -0.159136   -0.108800  \n",
       "GID6688934   -0.098509   -0.052304   -0.012778   -0.100318   -0.154557  \n",
       "...                ...         ...         ...         ...         ...  \n",
       "GID6939917    1.112390    0.077593    0.107666    0.121171    0.087428  \n",
       "GID6939919    0.077593    0.986131    0.159823    0.380622    0.167608  \n",
       "GID6939938    0.107666    0.159823    1.118190    0.388284    0.389739  \n",
       "GID6939941    0.121171    0.380622    0.388284    1.070441    0.219306  \n",
       "GID6939945    0.087428    0.167608    0.389739    0.219306    1.145606  \n",
       "\n",
       "[766 rows x 766 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the feature matrix and set the index to match y\n",
    "X = eyt1['Geno_Env1'].sort_index()\n",
    "\n",
    "# Display the feature matrix\n",
    "display(X)\n",
    "\n",
    "# Reset the index for both X and y to ensure they match\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c222e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_and_target(X: pd.DataFrame, y: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, StandardScaler, StandardScaler):\n",
    "    \"\"\"\n",
    "    Scale the feature matrix and target values using StandardScaler.\n",
    "\n",
    "    Created: 2024/01/01\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.DataFrame): Target values.\n",
    "    \n",
    "    Returns:\n",
    "        X_sc (pd.DataFrame): Scaled feature matrix.\n",
    "        y_sc (pd.DataFrame): Scaled target values.\n",
    "        X_scaler (StandardScaler): Scaler used for features.\n",
    "        y_scaler (StandardScaler): Scaler used for target.\n",
    "        \n",
    "    \"\"\"\n",
    "    X_sc, y_sc, X_scaler, y_scaler = None, None, None, None\n",
    "\n",
    "    # Scale the feature matrix\n",
    "    if X is not None:\n",
    "        X_scaler = StandardScaler()\n",
    "        X_sc = X_scaler.fit_transform(X)\n",
    "        X_sc = pd.DataFrame(X_sc, index=X.index, columns=X.columns)\n",
    "\n",
    "    # Scale the target values\n",
    "    if y is not None:\n",
    "        y_scaler = StandardScaler()\n",
    "        y_sc = y_scaler.fit_transform(y)\n",
    "        y_sc = pd.DataFrame(y_sc, index=y.index, columns=y.columns)\n",
    "    \n",
    "    return X_sc, y_sc, X_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11428d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smogn_prep(X: pd.DataFrame, y: pd.DataFrame, top_threshold_quantile: float, undersample: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset using the SMOGN algorithm\n",
    "    Created: 2024/02/05\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.DataFrame): DataFrame containing grain yield values\n",
    "        top_threshold_quantile (float): The quantile value to use as the threshold for the top class\n",
    "        undersample (bool): Whether to undersample the majority class (default is True)\n",
    "    \"\"\"\n",
    "\n",
    "    # Temporarily combine X and y for compatibility with the SMOGN library\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    smogn_X_y = pd.concat([X, y], axis=1)\n",
    "\n",
    "    # Get GY distribution points\n",
    "    gy_min = y['GY'].min()\n",
    "    gy_max = y['GY'].max()\n",
    "    gy_just_under_threshold = y['GY'].quantile(top_threshold_quantile - 0.0001)\n",
    "    gy_just_over_threshold = y['GY'].quantile(top_threshold_quantile + 0.0001)\n",
    "\n",
    "    # Define control points for the SMOGN augmentation relevance function\n",
    "    ctrl_points = [\n",
    "        [gy_min, 0, 0],\n",
    "        [gy_just_under_threshold, 0, 0],\n",
    "        [gy_just_over_threshold, 1, 0],\n",
    "        [gy_max, 1, 0]\n",
    "    ]\n",
    "\n",
    "    # Display the combined DataFrame\n",
    "    display(smogn_X_y)\n",
    "\n",
    "    n_tries = 0\n",
    "    done = False\n",
    "\n",
    "    # Unfortunately the library has a bug where it randomly throws exceptions occasionally regardless of input\n",
    "    # So we have to try multiple times until it works... this is hacky but it will have to do until the library is fixed\n",
    "    while not done:\n",
    "        try:\n",
    "            # Apply the SMOGN algorithm to balance the dataset\n",
    "            X = smoter(\n",
    "                data=smogn_X_y,\n",
    "                y='GY',\n",
    "                k=5,\n",
    "                under_samp=undersample,\n",
    "                samp_method='balance',\n",
    "                rel_thres=top_threshold_quantile,\n",
    "                rel_method='manual',\n",
    "                rel_ctrl_pts_rg=ctrl_points,\n",
    "                rel_xtrm_type='high',\n",
    "                rel_coef=1.50\n",
    "            )\n",
    "            done = True\n",
    "\n",
    "        except ValueError:\n",
    "            if n_tries < 5:\n",
    "                n_tries += 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # Split X and y back into separate DataFrames\n",
    "    y = X[['GY']]\n",
    "    X = X.drop(columns=['GY'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "589e47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the cutoff value for the \"Top Line\" classification of grain yield values\n",
    "top_boundary_val = y[\"GY\"].quantile(TOP_THRESHOLD_QUANTILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f8edf",
   "metadata": {},
   "source": [
    "# Model R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08cbb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_R(n_splits: int, X : pd.DataFrame, y : pd.DataFrame, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback, kfold_random_state: int, plot_title: str = \"\", **kwargs):\n",
    "    \"\"\"Perform inner cross-validation with grid search to find the best model parameters.\n",
    "    Created: 2024/12/03\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        Number of splits for KFold cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Parameter grid for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Parameter grid for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Callback function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for KFold shuffling.\n",
    "    plot_title : str, optional\n",
    "        Title for the plot (default is \"\").\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments for the model training.\n",
    "    Returns:\n",
    "    --------\n",
    "    avg_best_param1 : float\n",
    "        Average best parameter value for the first axis over all folds.\n",
    "    avg_best_param2 : float\n",
    "        Average best parameter value for the second axis over all folds.\"\"\"\n",
    "\n",
    "    # Create KFold object for inner-fold cross-validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Store best parameters (param1, param2) for each fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        \n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train a grid of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "\n",
    "        # Use trained models to predict test set labels, and store in 2D array with each cell corresponding to a model with a specific combination of parameters\n",
    "        y_preds_grid = bmo.grid_predict(X_test, model_grid)\n",
    "\n",
    "        # Create 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_test_grid = cdt.np_array_of_dfs(y_test, y_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating 2D array of Pearson coefficients\n",
    "        pearson_grid = bmo.calculate_pearson_coefficients(y_preds_grid, y_test_grid)\n",
    "\n",
    "        # Find index of best Pearson coefficient in the 2D array of Pearson coefficients\n",
    "        best_row, best_col = np.unravel_index(np.argmax(pearson_grid), pearson_grid.shape)\n",
    "        \n",
    "        # Store hyperparameters of the most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "\n",
    "        # Create grid of scatter plots with predictions vs actuals, colored by Pearson coefficient for each model\n",
    "        scatter_grid = plot_shaded_scatter_grids(y_preds_grid, y_test_grid, axis1_params, axis2_params, pearson_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\Model R {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "        plt.show(scatter_grid)\n",
    "        plt.close(scatter_grid)\n",
    "\n",
    "    # Calculate average best parameters over all inner folds to return to outer CV\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c111e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_R(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "               axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback : callable, random_state: int, top_boundary_val : float,\n",
    "               smogn_preprocess = False, undersamp_ratio = 1, oversamp_ratio = 1, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation with an outer and inner loop to evaluate model performance.\n",
    "    Created: 2024/12/03\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation loop.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation loop.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object containing parameter list for the first hyperparameter axis (horizontal).\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object containing parameter list for the second hyperparameter axis (vertical).\n",
    "    train_model_callback : callable\n",
    "        Function to train the model. Should accept X, y, and hyperparameters as arguments.\n",
    "    random_state : int\n",
    "        Random state for reproducibility in KFold splitting.\n",
    "    top_boundary_val : float\n",
    "        Threshold to classify predictions as top or not top.\n",
    "    smogn_preprocess : bool, optional\n",
    "        Whether to apply SMOGN preprocessing to the data. Default is False.\n",
    "    undersamp_ratio : float, optional\n",
    "        Proportion of the majority class to keep after undersampling. Default is 1.\n",
    "    oversamp_ratio : float, optional\n",
    "        Proportion of augmented data for the minority class to keep after oversampling. Default is 1.\n",
    "    **kwargs\n",
    "        Additional arguments to pass to the train_model_callback function.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing 5 metrics (Pearson, F1 Score, Sensitivity, Specificity, Kappa) for each outer fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create KFold object for outer loop to split data into train and test sets\n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    kfold_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "    # Create arrays to store outer-fold final \"super model\"'s predictions and actuals\n",
    "    super_model_preds = [None] * n_outer_splits\n",
    "    super_model_actuals = [None] * n_outer_splits \n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Display the training and test data\n",
    "        print(f\"Outer Fold {i} Training and Test Data:\")\n",
    "        print(\"X_train:\")\n",
    "        display(X_train)\n",
    "        print(\"y_train:\")\n",
    "        display(y_train)\n",
    "        print(\"X_test:\")\n",
    "        display(X_test)\n",
    "        print(\"y_test:\")\n",
    "        display(y_test)\n",
    "\n",
    "        if smogn_preprocess:\n",
    "\n",
    "            ### TODO ###\n",
    "            top_quant_train = percentileofscore(y_train.to_numpy().flatten(), top_boundary_val, kind='mean') / 100\n",
    "\n",
    "            X_train_top_smog, y_train_top_smog = smogn_prep(X_train, y_train, top_quant_train, False) # We won't use SMOGN's built-in undersampling because we will do it manually later\n",
    "\n",
    "            # Undersample the majority class so that it is a certain proportion of its original size\n",
    "            #X_train_us, y_train_us = cdt.random_subset(X_train, y_train, random_state=random_state)\n",
    "            \n",
    "            # Split the augmented and original data into seperate columns for plotting\n",
    "            original_data = pra.intersection(y_train, y_train_top_smog)\n",
    "            augmented_data = pra.difference(y_train_top_smog, original_data)\n",
    "            orig_aug_data = pd.concat([original_data, augmented_data], axis=1)\n",
    "            orig_aug_data.columns = ['Original GY', 'Augmented GY']\n",
    "\n",
    "            # Manually concatenate the original data below the augmentation threshold with the augmented data\n",
    "            non_augmented_indices = y_train[y_train < top_boundary_val].index\n",
    "            X_train_non_augmented = X_train.loc[non_augmented_indices]\n",
    "            y_train_non_augmented = y_train.loc[non_augmented_indices]\n",
    "            X_train_top_smog = pd.concat([X_train_top_smog, X_train_non_augmented], axis=0)\n",
    "            y_train_top_smog = pd.concat([y_train_top_smog, y_train_non_augmented], axis=0)\n",
    "\n",
    "            histogram(orig_aug_data, f'Model R SMOGN-Augmented GY Histogram, Outer Fold {i}', vline_value=top_boundary_val)\n",
    "\n",
    "            ### TODO ###\n",
    "\n",
    "            top_quant_train = percentileofscore(y_train.to_numpy().flatten(), top_boundary_val, kind='mean') / 100\n",
    "\n",
    "            X_train_top_smog, y_train_top_smog = smogn_prep(X_train, y_train, top_quant_train, False) # We won't use SMOGN's built-in undersampling because we will do it manually later\n",
    "            print(\"After SMOGN prep:\")\n",
    "            display(X_train_top_smog)\n",
    "            display(y_train_top_smog)\n",
    "\n",
    "            # Split the augmented and original top line data into separate columns for plotting\n",
    "            y_train_top_smog_orig = pra.intersection(y_train, y_train_top_smog)\n",
    "            X_train_top_smog_orig = X_train_top_smog.loc[y_train_top_smog_orig.index]\n",
    "            y_train_top_smog_aug = pra.difference(y_train_top_smog, y_train_top_smog_orig)\n",
    "            X_train_top_smog_aug = X_train_top_smog.loc[y_train_top_smog_aug.index]\n",
    "            print(\"After splitting augmented and original top line data:\")\n",
    "            display(X_train_top_smog_orig)\n",
    "            display(y_train_top_smog_orig)\n",
    "            display(X_train_top_smog_aug)\n",
    "            display(y_train_top_smog_aug)\n",
    "\n",
    "            # Randomly undersample the augmented data\n",
    "            X_train_top_smog_aug_us, y_train_top_smog_aug_us = cdt.random_subset(X_train_top_smog_aug, y_train_top_smog_aug, p=oversamp_ratio, random_state=random_state)\n",
    "            print(\"After undersampling augmented data:\")\n",
    "            display(X_train_top_smog_aug_us)\n",
    "            display(y_train_top_smog_aug_us)\n",
    "\n",
    "            # Plot histograms of original and augmented GY values stacked\n",
    "            orig_aug_data = pd.concat([y_train_top_smog_orig, y_train_top_smog_aug_us], axis=1)\n",
    "            orig_aug_data.columns = ['Original GY', 'Augmented GY']\n",
    "            histogram(orig_aug_data, f'Model R SMOGN-Augmented GY Histogram, Outer Fold {i}', vline_value=top_boundary_val)\n",
    "\n",
    "            # Re-combine the original and augmented top line data\n",
    "            X_train_top_final = pd.concat([X_train_top_smog_orig, X_train_top_smog_aug_us], axis=0)\n",
    "            y_train_top_final = pd.concat([y_train_top_smog_orig, y_train_top_smog_aug_us], axis=0)\n",
    "            print(\"After recombining original and augmented top line data:\")\n",
    "            display(X_train_top_final)\n",
    "            display(y_train_top_final)\n",
    "\n",
    "            # Undersample the majority class so that it is a certain proportion of its original size\n",
    "            X_train_maj, y_train_maj = X_train.loc[y_train[y_train < top_boundary_val].index], y_train.loc[y_train[y_train < top_boundary_val].index]\n",
    "            X_train_maj_us, y_train_maj_us = cdt.random_subset(X_train_maj, y_train_maj, p=undersamp_ratio, random_state=random_state)\n",
    "            print(\"After undersampling majority class:\")\n",
    "            display(X_train_maj_us)\n",
    "            display(y_train_maj_us)\n",
    "\n",
    "            X_train_final = pd.concat([X_train_top_final, X_train_maj_us], axis=0)\n",
    "            y_train_final = pd.concat([y_train_top_final, y_train_maj_us], axis=0)\n",
    "            print(\"Final training data after combining top and majority class data:\")\n",
    "            display(X_train_final)\n",
    "            display(y_train_final)\n",
    "\n",
    "        else:\n",
    "            histogram(y_train, f'Model R Histogram, Outer Fold {i}', vline_value=top_boundary_val)\n",
    "\n",
    "        # Display summary statistics of y_train\n",
    "        display(y_train_final.describe())\n",
    "\n",
    "        # Scale features and target\n",
    "        X_train_final, y_train_final, X_scaler, y_scaler = scale_features_and_target(X_train_final, y_train_final)\n",
    "        top_boundary_val_scaled = y_scaler.transform([[top_boundary_val]])[0, 0]\n",
    "        X_test = pd.DataFrame(X_scaler.transform(X_test))\n",
    "        y_test = pd.DataFrame(y_scaler.transform(y_test))\n",
    "            \n",
    "        # Find mean best hyperparameter values based on prediction accuracy using inner-fold CV\n",
    "        best_param1, best_param2 = inner_CV_R(n_inner_splits, X_train_final, y_train_final, axis1_params, axis2_params, train_model_callback, random_state, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train_final, np.ravel(y_train_final), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set\n",
    "        X_test.columns = X.columns\n",
    "        y_pred = pd.DataFrame(super_model.predict(X_test), index=y_test.index, columns=y_test.columns)\n",
    "        histogram(y_pred, f'Model R Predicted GY Histogram, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "\n",
    "        # Calculate Pearson coefficient of continuous predictions\n",
    "        pearson, _ = pearsonr(np.ravel(y_pred), np.ravel(y_test))\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean)\n",
    "        y_pred_top = bmo.continuous_to_binary_absolute(y_pred, top_boundary_val_scaled)\n",
    "        super_model_preds[i] = y_pred_top\n",
    "        y_test_top = bmo.continuous_to_binary_absolute(y_test, top_boundary_val_scaled)\n",
    "        super_model_actuals[i] = y_test_top\n",
    "\n",
    "        # Plot super model predictions vs actuals scatterplot\n",
    "        cmp.plot_classification_results(y_pred, y_test, y_pred_top, y_test_top, \n",
    "                                        [f\"Model R Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}\"],\n",
    "                                        save_path=f'{storage_dir}\\\\Model R Super Model Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}.svg')\n",
    "\n",
    "        # Calculate classification metrics and add new row to kfold_metrics\n",
    "        classification_metrics = cdt.classification_metrics(y_pred_top, y_test_top)\n",
    "        pearson_df = pd.DataFrame([pearson], columns=['Pearson'])\n",
    "        metrics_row = pd.concat([pearson_df, classification_metrics], axis=1)\n",
    "        kfold_metrics = pd.concat([kfold_metrics, metrics_row], axis=0)\n",
    "\n",
    "    # Label each row of kfold_metrics with the fold number \n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    \n",
    "    return kfold_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40ce7c",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a31a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 0 Training and Test Data:\n",
      "X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939899</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788801</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.157880</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>-0.110899</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.199459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-0.201346</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072171</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.004447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>0.402479</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.102718</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079312</td>\n",
       "      <td>-0.087824</td>\n",
       "      <td>-0.089912</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.157880</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.996666</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.395843</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>-0.138902</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.026372</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>-0.154557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.942013</td>\n",
       "      <td>-0.064273</td>\n",
       "      <td>0.116772</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101016</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.152167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>-0.186001</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>-0.048967</td>\n",
       "      <td>0.136133</td>\n",
       "      <td>...</td>\n",
       "      <td>1.182832</td>\n",
       "      <td>1.180574</td>\n",
       "      <td>1.163339</td>\n",
       "      <td>1.229832</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.639670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>-0.191481</td>\n",
       "      <td>0.034453</td>\n",
       "      <td>-0.045460</td>\n",
       "      <td>0.136212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164231</td>\n",
       "      <td>1.170088</td>\n",
       "      <td>1.157341</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>1.266578</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.649749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.149919</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719743</td>\n",
       "      <td>0.728707</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>1.118190</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.389739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>-0.206353</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.084022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465269</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.473004</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>1.070441</td>\n",
       "      <td>0.219306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>1.145606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  GID6688949  \\\n",
       "0      0.788801   -0.006443    0.025987   -0.138795   -0.157880    0.096213   \n",
       "1     -0.006443    0.980542    0.064585   -0.168773   -0.081006    0.078890   \n",
       "3     -0.138795   -0.168773   -0.021636    0.879004    0.443678   -0.080560   \n",
       "4     -0.157880   -0.081006   -0.031717    0.443678    0.996666   -0.140766   \n",
       "5      0.096213    0.078890    0.101532   -0.080560   -0.140766    0.942013   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "759    0.127674    0.079085    0.408326   -0.067028   -0.014478    0.097538   \n",
       "760    0.130468    0.061086    0.426844   -0.084206   -0.016350    0.099849   \n",
       "763    0.074009    0.108757    0.240468   -0.096740   -0.012778    0.058980   \n",
       "764    0.032992    0.154718    0.255337   -0.159136   -0.100318    0.045545   \n",
       "765    0.199459    0.004447    0.163524   -0.108800   -0.154557    0.152167   \n",
       "\n",
       "     GID6689407  GID6689482  GID6689550  GID6738288  ...  GID6939899  \\\n",
       "0     -0.110899    0.013069   -0.040445    0.007931  ...    0.125612   \n",
       "1     -0.201346    0.124671    0.253505    0.013636  ...    0.072171   \n",
       "3      0.402479   -0.218803   -0.102718   -0.002303  ...   -0.079312   \n",
       "4      0.395843   -0.310471   -0.138902    0.088169  ...   -0.016690   \n",
       "5     -0.064273    0.116772    0.071070   -0.016037  ...    0.101016   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "759   -0.186001    0.034795   -0.048967    0.136133  ...    1.182832   \n",
       "760   -0.191481    0.034453   -0.045460    0.136212  ...    1.164231   \n",
       "763   -0.149919    0.000392    0.063573    0.090066  ...    0.719743   \n",
       "764   -0.206353    0.194754    0.043889    0.084022  ...    0.465269   \n",
       "765   -0.163107    0.160584    0.030285    0.010552  ...    0.660434   \n",
       "\n",
       "     GID6939900  GID6939902  GID6939903  GID6939904  GID6939917  GID6939919  \\\n",
       "0      0.133808    0.137456    0.127674    0.130468    0.004096    0.091188   \n",
       "1      0.061650    0.057898    0.079085    0.061086    0.104630    0.113878   \n",
       "3     -0.087824   -0.089912   -0.067028   -0.084206   -0.140529   -0.088961   \n",
       "4     -0.017375   -0.026372   -0.014478   -0.016350   -0.098509   -0.052304   \n",
       "5      0.094683    0.093677    0.097538    0.099849    0.048248    0.139241   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "759    1.180574    1.163339    1.229832    1.142452    0.150037    0.242206   \n",
       "760    1.170088    1.157341    1.142452    1.266578    0.161459    0.253641   \n",
       "763    0.728707    0.732558    0.708062    0.704652    0.107666    0.159823   \n",
       "764    0.461691    0.473004    0.432339    0.435412    0.121171    0.380622   \n",
       "765    0.664936    0.645452    0.639670    0.649749    0.087428    0.167608   \n",
       "\n",
       "     GID6939938  GID6939941  GID6939945  \n",
       "0      0.074009    0.032992    0.199459  \n",
       "1      0.108757    0.154718    0.004447  \n",
       "3     -0.096740   -0.159136   -0.108800  \n",
       "4     -0.012778   -0.100318   -0.154557  \n",
       "5      0.058980    0.045545    0.152167  \n",
       "..          ...         ...         ...  \n",
       "759    0.708062    0.432339    0.639670  \n",
       "760    0.704652    0.435412    0.649749  \n",
       "763    1.118190    0.388284    0.389739  \n",
       "764    0.388284    1.070441    0.219306  \n",
       "765    0.389739    0.219306    1.145606  \n",
       "\n",
       "[612 rows x 766 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.160521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.988963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.434369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.551610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.459668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>5.468315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>5.307925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>5.487924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>5.332815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.510633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           GY\n",
       "0    5.160521\n",
       "1    5.988963\n",
       "3    5.434369\n",
       "4    5.551610\n",
       "5    5.459668\n",
       "..        ...\n",
       "759  5.468315\n",
       "760  5.307925\n",
       "763  5.487924\n",
       "764  5.332815\n",
       "765  5.510633\n",
       "\n",
       "[612 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939899</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025987</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>1.170073</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.196780</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>-0.013459</td>\n",
       "      <td>0.126464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428609</td>\n",
       "      <td>0.423184</td>\n",
       "      <td>0.427788</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>0.163524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>0.116772</td>\n",
       "      <td>-0.203377</td>\n",
       "      <td>1.007993</td>\n",
       "      <td>0.107034</td>\n",
       "      <td>-0.070960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>0.034453</td>\n",
       "      <td>0.062388</td>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.160584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.109871</td>\n",
       "      <td>-0.043212</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>-0.080362</td>\n",
       "      <td>-0.123606</td>\n",
       "      <td>0.091173</td>\n",
       "      <td>-0.093000</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.041180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.096431</td>\n",
       "      <td>-0.060974</td>\n",
       "      <td>0.039714</td>\n",
       "      <td>-0.037005</td>\n",
       "      <td>0.111964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.172901</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>-0.052621</td>\n",
       "      <td>-0.123805</td>\n",
       "      <td>0.030219</td>\n",
       "      <td>-0.164872</td>\n",
       "      <td>-0.037137</td>\n",
       "      <td>-0.007355</td>\n",
       "      <td>-0.060900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005307</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.001822</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>-0.003271</td>\n",
       "      <td>-0.035701</td>\n",
       "      <td>-0.010477</td>\n",
       "      <td>-0.052301</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.076113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.292383</td>\n",
       "      <td>-0.051311</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>-0.192819</td>\n",
       "      <td>-0.113803</td>\n",
       "      <td>-0.029163</td>\n",
       "      <td>-0.119407</td>\n",
       "      <td>-0.054013</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>0.043735</td>\n",
       "      <td>0.034872</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>-0.029913</td>\n",
       "      <td>-0.038769</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.020876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.066207</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>0.133376</td>\n",
       "      <td>-0.168064</td>\n",
       "      <td>-0.249350</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>-0.258948</td>\n",
       "      <td>0.254711</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.085440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277748</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.272966</td>\n",
       "      <td>0.270166</td>\n",
       "      <td>0.254486</td>\n",
       "      <td>0.084954</td>\n",
       "      <td>0.161791</td>\n",
       "      <td>0.207099</td>\n",
       "      <td>0.485994</td>\n",
       "      <td>0.210648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.039663</td>\n",
       "      <td>0.326185</td>\n",
       "      <td>-0.122592</td>\n",
       "      <td>-0.051694</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>-0.194428</td>\n",
       "      <td>0.213666</td>\n",
       "      <td>-0.049605</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666623</td>\n",
       "      <td>0.673941</td>\n",
       "      <td>0.655706</td>\n",
       "      <td>0.648912</td>\n",
       "      <td>0.673782</td>\n",
       "      <td>0.101335</td>\n",
       "      <td>0.170709</td>\n",
       "      <td>0.381623</td>\n",
       "      <td>0.269217</td>\n",
       "      <td>0.356573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.072171</td>\n",
       "      <td>0.428609</td>\n",
       "      <td>-0.079312</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>0.101016</td>\n",
       "      <td>-0.192171</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>-0.062084</td>\n",
       "      <td>0.133040</td>\n",
       "      <td>...</td>\n",
       "      <td>1.273587</td>\n",
       "      <td>1.213512</td>\n",
       "      <td>1.205270</td>\n",
       "      <td>1.182832</td>\n",
       "      <td>1.164231</td>\n",
       "      <td>0.144931</td>\n",
       "      <td>0.255040</td>\n",
       "      <td>0.719743</td>\n",
       "      <td>0.465269</td>\n",
       "      <td>0.660434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>-0.114305</td>\n",
       "      <td>0.062388</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144931</td>\n",
       "      <td>0.144932</td>\n",
       "      <td>0.155032</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>1.112390</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.087428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>-0.141205</td>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.070163</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255040</td>\n",
       "      <td>0.249602</td>\n",
       "      <td>0.262775</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.986131</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.167608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  GID6688949  \\\n",
       "2      0.025987    0.064585    1.170073   -0.021636   -0.031717    0.101532   \n",
       "7      0.013069    0.124671    0.041900   -0.218803   -0.310471    0.116772   \n",
       "10     0.109871   -0.043212    0.020647   -0.080362   -0.123606    0.091173   \n",
       "23     0.172901    0.016505    0.004827   -0.052621   -0.123805    0.030219   \n",
       "30     0.292383   -0.051311   -0.028310   -0.192819   -0.113803   -0.029163   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "752    0.066207    0.101028    0.133376   -0.168064   -0.249350    0.061883   \n",
       "753    0.029349    0.039663    0.326185   -0.122592   -0.051694    0.046551   \n",
       "756    0.125612    0.072171    0.428609   -0.079312   -0.016690    0.101016   \n",
       "761    0.004096    0.104630    0.006038   -0.140529   -0.098509    0.048248   \n",
       "762    0.091188    0.113878    0.209395   -0.088961   -0.052304    0.139241   \n",
       "\n",
       "     GID6689407  GID6689482  GID6689550  GID6738288  ...  GID6939899  \\\n",
       "2     -0.196780    0.041900   -0.013459    0.126464  ...    0.428609   \n",
       "7     -0.203377    1.007993    0.107034   -0.070960  ...    0.038721   \n",
       "10    -0.093000   -0.005524    0.009856    0.041180  ...    0.015627   \n",
       "23    -0.164872   -0.037137   -0.007355   -0.060900  ...   -0.005307   \n",
       "30    -0.119407   -0.054013   -0.202637    0.018180  ...    0.047542   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "752   -0.258948    0.254711    0.004110   -0.085440  ...    0.277748   \n",
       "753   -0.194428    0.213666   -0.049605    0.044506  ...    0.666623   \n",
       "756   -0.192171    0.038721   -0.062084    0.133040  ...    1.273587   \n",
       "761   -0.114305    0.062388    0.060255    0.034630  ...    0.144931   \n",
       "762   -0.141205    0.184798    0.070163    0.015030  ...    0.255040   \n",
       "\n",
       "     GID6939900  GID6939902  GID6939903  GID6939904  GID6939917  GID6939919  \\\n",
       "2      0.423184    0.427788    0.408326    0.426844    0.006038    0.209395   \n",
       "7      0.027326    0.044509    0.034795    0.034453    0.062388    0.184798   \n",
       "10     0.005989    0.004205    0.015294   -0.010071   -0.096431   -0.060974   \n",
       "23    -0.004725   -0.001822    0.004854   -0.003271   -0.035701   -0.010477   \n",
       "30     0.052870    0.043735    0.034872    0.046155   -0.029913   -0.038769   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "752    0.283144    0.272966    0.270166    0.254486    0.084954    0.161791   \n",
       "753    0.673941    0.655706    0.648912    0.673782    0.101335    0.170709   \n",
       "756    1.213512    1.205270    1.182832    1.164231    0.144931    0.255040   \n",
       "761    0.144932    0.155032    0.150037    0.161459    1.112390    0.077593   \n",
       "762    0.249602    0.262775    0.242206    0.253641    0.077593    0.986131   \n",
       "\n",
       "     GID6939938  GID6939941  GID6939945  \n",
       "2      0.240468    0.255337    0.163524  \n",
       "7      0.000392    0.194754    0.160584  \n",
       "10     0.039714   -0.037005    0.111964  \n",
       "23    -0.052301    0.000325    0.076113  \n",
       "30     0.005638    0.001935    0.020876  \n",
       "..          ...         ...         ...  \n",
       "752    0.207099    0.485994    0.210648  \n",
       "753    0.381623    0.269217    0.356573  \n",
       "756    0.719743    0.465269    0.660434  \n",
       "761    0.107666    0.121171    0.087428  \n",
       "762    0.159823    0.380622    0.167608  \n",
       "\n",
       "[154 rows x 766 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.781745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.700342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.075788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.769053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.407887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>5.189311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>5.630265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>5.582883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>5.505734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>5.501248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           GY\n",
       "2    5.781745\n",
       "7    5.700342\n",
       "10   5.075788\n",
       "23   4.769053\n",
       "30   5.407887\n",
       "..        ...\n",
       "752  5.189311\n",
       "753  5.630265\n",
       "756  5.582883\n",
       "761  5.505734\n",
       "762  5.501248\n",
       "\n",
       "[154 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788801</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.157880</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>-0.110899</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.199459</td>\n",
       "      <td>5.160521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-0.201346</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>5.988963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>0.402479</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.102718</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087824</td>\n",
       "      <td>-0.089912</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>5.434369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.157880</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.996666</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.395843</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>-0.138902</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.026372</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>5.551610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.942013</td>\n",
       "      <td>-0.064273</td>\n",
       "      <td>0.116772</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>5.459668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>-0.186001</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>-0.048967</td>\n",
       "      <td>0.136133</td>\n",
       "      <td>...</td>\n",
       "      <td>1.180574</td>\n",
       "      <td>1.163339</td>\n",
       "      <td>1.229832</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>5.468315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>-0.191481</td>\n",
       "      <td>0.034453</td>\n",
       "      <td>-0.045460</td>\n",
       "      <td>0.136212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170088</td>\n",
       "      <td>1.157341</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>1.266578</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>5.307925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.149919</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728707</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>1.118190</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>5.487924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>-0.206353</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.084022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.473004</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>1.070441</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>5.332815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>1.145606</td>\n",
       "      <td>5.510633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 767 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  GID6688949  \\\n",
       "0      0.788801   -0.006443    0.025987   -0.138795   -0.157880    0.096213   \n",
       "1     -0.006443    0.980542    0.064585   -0.168773   -0.081006    0.078890   \n",
       "2     -0.138795   -0.168773   -0.021636    0.879004    0.443678   -0.080560   \n",
       "3     -0.157880   -0.081006   -0.031717    0.443678    0.996666   -0.140766   \n",
       "4      0.096213    0.078890    0.101532   -0.080560   -0.140766    0.942013   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "607    0.127674    0.079085    0.408326   -0.067028   -0.014478    0.097538   \n",
       "608    0.130468    0.061086    0.426844   -0.084206   -0.016350    0.099849   \n",
       "609    0.074009    0.108757    0.240468   -0.096740   -0.012778    0.058980   \n",
       "610    0.032992    0.154718    0.255337   -0.159136   -0.100318    0.045545   \n",
       "611    0.199459    0.004447    0.163524   -0.108800   -0.154557    0.152167   \n",
       "\n",
       "     GID6689407  GID6689482  GID6689550  GID6738288  ...  GID6939900  \\\n",
       "0     -0.110899    0.013069   -0.040445    0.007931  ...    0.133808   \n",
       "1     -0.201346    0.124671    0.253505    0.013636  ...    0.061650   \n",
       "2      0.402479   -0.218803   -0.102718   -0.002303  ...   -0.087824   \n",
       "3      0.395843   -0.310471   -0.138902    0.088169  ...   -0.017375   \n",
       "4     -0.064273    0.116772    0.071070   -0.016037  ...    0.094683   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "607   -0.186001    0.034795   -0.048967    0.136133  ...    1.180574   \n",
       "608   -0.191481    0.034453   -0.045460    0.136212  ...    1.170088   \n",
       "609   -0.149919    0.000392    0.063573    0.090066  ...    0.728707   \n",
       "610   -0.206353    0.194754    0.043889    0.084022  ...    0.461691   \n",
       "611   -0.163107    0.160584    0.030285    0.010552  ...    0.664936   \n",
       "\n",
       "     GID6939902  GID6939903  GID6939904  GID6939917  GID6939919  GID6939938  \\\n",
       "0      0.137456    0.127674    0.130468    0.004096    0.091188    0.074009   \n",
       "1      0.057898    0.079085    0.061086    0.104630    0.113878    0.108757   \n",
       "2     -0.089912   -0.067028   -0.084206   -0.140529   -0.088961   -0.096740   \n",
       "3     -0.026372   -0.014478   -0.016350   -0.098509   -0.052304   -0.012778   \n",
       "4      0.093677    0.097538    0.099849    0.048248    0.139241    0.058980   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "607    1.163339    1.229832    1.142452    0.150037    0.242206    0.708062   \n",
       "608    1.157341    1.142452    1.266578    0.161459    0.253641    0.704652   \n",
       "609    0.732558    0.708062    0.704652    0.107666    0.159823    1.118190   \n",
       "610    0.473004    0.432339    0.435412    0.121171    0.380622    0.388284   \n",
       "611    0.645452    0.639670    0.649749    0.087428    0.167608    0.389739   \n",
       "\n",
       "     GID6939941  GID6939945        GY  \n",
       "0      0.032992    0.199459  5.160521  \n",
       "1      0.154718    0.004447  5.988963  \n",
       "2     -0.159136   -0.108800  5.434369  \n",
       "3     -0.100318   -0.154557  5.551610  \n",
       "4      0.045545    0.152167  5.459668  \n",
       "..          ...         ...       ...  \n",
       "607    0.432339    0.639670  5.468315  \n",
       "608    0.435412    0.649749  5.307925  \n",
       "609    0.388284    0.389739  5.487924  \n",
       "610    1.070441    0.219306  5.332815  \n",
       "611    0.219306    1.145606  5.510633  \n",
       "\n",
       "[612 rows x 767 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_matrix: 100%|##########| 122/122 [01:13<00:00,  1.67it/s]\n",
      "synth_matrix: 100%|##########| 122/122 [00:07<00:00, 16.07it/s]\n",
      "r_index: 100%|##########| 61/61 [00:03<00:00, 16.61it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788801</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.157880</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>-0.110899</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.199459</td>\n",
       "      <td>5.160521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-0.201346</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>5.988963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>0.402479</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.102718</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087824</td>\n",
       "      <td>-0.089912</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>5.434369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.157880</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.996666</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.395843</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>-0.138902</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.026372</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>5.551610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.942013</td>\n",
       "      <td>-0.064273</td>\n",
       "      <td>0.116772</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>5.459668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>-0.186001</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>-0.048967</td>\n",
       "      <td>0.136133</td>\n",
       "      <td>...</td>\n",
       "      <td>1.180574</td>\n",
       "      <td>1.163339</td>\n",
       "      <td>1.229832</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>5.468315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>-0.191481</td>\n",
       "      <td>0.034453</td>\n",
       "      <td>-0.045460</td>\n",
       "      <td>0.136212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170088</td>\n",
       "      <td>1.157341</td>\n",
       "      <td>1.142452</td>\n",
       "      <td>1.266578</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>5.307925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.149919</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728707</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>1.118190</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>5.487924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>-0.206353</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.084022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.473004</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>1.070441</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>5.332815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>1.145606</td>\n",
       "      <td>5.510633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 767 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  GID6688949  \\\n",
       "0      0.788801   -0.006443    0.025987   -0.138795   -0.157880    0.096213   \n",
       "1     -0.006443    0.980542    0.064585   -0.168773   -0.081006    0.078890   \n",
       "2     -0.138795   -0.168773   -0.021636    0.879004    0.443678   -0.080560   \n",
       "3     -0.157880   -0.081006   -0.031717    0.443678    0.996666   -0.140766   \n",
       "4      0.096213    0.078890    0.101532   -0.080560   -0.140766    0.942013   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "607    0.127674    0.079085    0.408326   -0.067028   -0.014478    0.097538   \n",
       "608    0.130468    0.061086    0.426844   -0.084206   -0.016350    0.099849   \n",
       "609    0.074009    0.108757    0.240468   -0.096740   -0.012778    0.058980   \n",
       "610    0.032992    0.154718    0.255337   -0.159136   -0.100318    0.045545   \n",
       "611    0.199459    0.004447    0.163524   -0.108800   -0.154557    0.152167   \n",
       "\n",
       "     GID6689407  GID6689482  GID6689550  GID6738288  ...  GID6939900  \\\n",
       "0     -0.110899    0.013069   -0.040445    0.007931  ...    0.133808   \n",
       "1     -0.201346    0.124671    0.253505    0.013636  ...    0.061650   \n",
       "2      0.402479   -0.218803   -0.102718   -0.002303  ...   -0.087824   \n",
       "3      0.395843   -0.310471   -0.138902    0.088169  ...   -0.017375   \n",
       "4     -0.064273    0.116772    0.071070   -0.016037  ...    0.094683   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "607   -0.186001    0.034795   -0.048967    0.136133  ...    1.180574   \n",
       "608   -0.191481    0.034453   -0.045460    0.136212  ...    1.170088   \n",
       "609   -0.149919    0.000392    0.063573    0.090066  ...    0.728707   \n",
       "610   -0.206353    0.194754    0.043889    0.084022  ...    0.461691   \n",
       "611   -0.163107    0.160584    0.030285    0.010552  ...    0.664936   \n",
       "\n",
       "     GID6939902  GID6939903  GID6939904  GID6939917  GID6939919  GID6939938  \\\n",
       "0      0.137456    0.127674    0.130468    0.004096    0.091188    0.074009   \n",
       "1      0.057898    0.079085    0.061086    0.104630    0.113878    0.108757   \n",
       "2     -0.089912   -0.067028   -0.084206   -0.140529   -0.088961   -0.096740   \n",
       "3     -0.026372   -0.014478   -0.016350   -0.098509   -0.052304   -0.012778   \n",
       "4      0.093677    0.097538    0.099849    0.048248    0.139241    0.058980   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "607    1.163339    1.229832    1.142452    0.150037    0.242206    0.708062   \n",
       "608    1.157341    1.142452    1.266578    0.161459    0.253641    0.704652   \n",
       "609    0.732558    0.708062    0.704652    0.107666    0.159823    1.118190   \n",
       "610    0.473004    0.432339    0.435412    0.121171    0.380622    0.388284   \n",
       "611    0.645452    0.639670    0.649749    0.087428    0.167608    0.389739   \n",
       "\n",
       "     GID6939941  GID6939945        GY  \n",
       "0      0.032992    0.199459  5.160521  \n",
       "1      0.154718    0.004447  5.988963  \n",
       "2     -0.159136   -0.108800  5.434369  \n",
       "3     -0.100318   -0.154557  5.551610  \n",
       "4      0.045545    0.152167  5.459668  \n",
       "..          ...         ...       ...  \n",
       "607    0.432339    0.639670  5.468315  \n",
       "608    0.435412    0.649749  5.307925  \n",
       "609    0.388284    0.389739  5.487924  \n",
       "610    1.070441    0.219306  5.332815  \n",
       "611    0.219306    1.145606  5.510633  \n",
       "\n",
       "[612 rows x 767 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_matrix:  62%|######2   | 76/122 [00:56<00:34,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x_params_SVM_R \u001b[38;5;241m=\u001b[39m mdo\u001b[38;5;241m.\u001b[39mAxisParams(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m, bmo\u001b[38;5;241m.\u001b[39mpower_list(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      3\u001b[0m y_params_SVM_R \u001b[38;5;241m=\u001b[39m mdo\u001b[38;5;241m.\u001b[39mAxisParams(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, bmo\u001b[38;5;241m.\u001b[39mpower_list(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m metrics_SVM_R \u001b[38;5;241m=\u001b[39m \u001b[43mouter_CV_R\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_outer_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mn_inner_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                           \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43maxis1_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_params_SVM_R\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                           \u001b[49m\u001b[43maxis2_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_params_SVM_R\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtrain_model_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_SVM_regressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msmogn_preprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSMOGN_PREPROCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtop_boundary_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_boundary_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mundersamp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                           \u001b[49m\u001b[43moversamp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrbf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m# Dummy values - Uncomment for quick debugging tests  \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mx_params_SVM_R = mdo.AxisParams('gamma', bmo.power_list(2, -1, 0))  \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m                           kernel='rbf')  \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 99\u001b[0m, in \u001b[0;36mouter_CV_R\u001b[1;34m(n_outer_splits, n_inner_splits, X, y, axis1_params, axis2_params, train_model_callback, random_state, top_boundary_val, smogn_preprocess, undersamp_ratio, oversamp_ratio, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m### TODO ###\u001b[39;00m\n\u001b[0;32m     97\u001b[0m top_quant_train \u001b[38;5;241m=\u001b[39m percentileofscore(y_train\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mflatten(), top_boundary_val, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 99\u001b[0m X_train_top_smog, y_train_top_smog \u001b[38;5;241m=\u001b[39m \u001b[43msmogn_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_quant_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# We won't use SMOGN's built-in undersampling because we will do it manually later\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter SMOGN prep:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m display(X_train_top_smog)\n",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m, in \u001b[0;36msmogn_prep\u001b[1;34m(X, y, top_threshold_quantile, undersample)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# Apply the SMOGN algorithm to balance the dataset\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43msmoter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmogn_X_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43munder_samp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mundersample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43msamp_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrel_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_threshold_quantile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrel_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmanual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrel_ctrl_pts_rg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctrl_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrel_xtrm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrel_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.50\u001b[39;49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\smogn\\smoter.py:240\u001b[0m, in \u001b[0;36msmoter\u001b[1;34m(data, y, k, pert, samp_method, under_samp, drop_na_col, drop_na_row, replace, rel_thres, rel_method, rel_xtrm_type, rel_coef, rel_ctrl_pts_rg)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m## over-sampling\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s_perc[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    236\u001b[0m     \n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m## generate synthetic observations in training set\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m## considered 'minority'\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m## (see 'over_sampling()' function for details)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     synth_obs \u001b[38;5;241m=\u001b[39m \u001b[43mover_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms_perc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpert\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m## concatenate over-sampling\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;66;03m## results to modified training set\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     data_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([synth_obs, data_new])\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\smogn\\over_sampling.py:169\u001b[0m, in \u001b[0;36mover_sampling\u001b[1;34m(data, index, perc, pert, k)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m    165\u001b[0m     \n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m## utilize euclidean distance given that \u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m## data is all numeric / continuous\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feat_count_nom \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m         dist_matrix[i][j] \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_dist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_num\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_num\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeat_count_num\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m## utilize heom distance given that \u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m## data contains both numeric / continuous \u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m## and nominal / categorical\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feat_count_nom \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m feat_count_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\smogn\\dist_metrics.py:21\u001b[0m, in \u001b[0;36meuclidean_dist\u001b[1;34m(a, b, d)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## loop through columns to calculate euclidean \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m## distance for numeric / continuous features\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d):\n\u001b[0;32m     18\u001b[0m     \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m## the squared difference of values in\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m## vectors a and b of equal length \u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     dist[i] \u001b[38;5;241m=\u001b[39m (\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m b\u001b[38;5;241m.\u001b[39miloc[i]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m## sum all the squared differences and take the square root\u001b[39;00m\n\u001b[0;32m     24\u001b[0m dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28msum\u001b[39m(dist))\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1737\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   1735\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key)\n\u001b[1;32m-> 1737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\pandas\\core\\common.py:125\u001b[0m, in \u001b[0;36mis_bool_indexer\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_bool_indexer\u001b[39m(key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Check whether `key` is a valid boolean indexer.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        and convert to an ndarray.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    126\u001b[0m         key, (ABCSeries, np\u001b[38;5;241m.\u001b[39mndarray, ABCIndex, ABCExtensionArray)\n\u001b[0;32m    127\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ABCMultiIndex):\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    129\u001b[0m             key_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:44\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[1;34m(cls, inst)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:37\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._check\u001b[1;34m(inst)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_pandas_abc_type\u001b[39m(name, attr, comp):\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check\u001b[39m(inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_typ\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define hyperparameter grids for SVM regression model and conduct 2-dimensional cross-validation\n",
    "x_params_SVM_R = mdo.AxisParams('gamma', bmo.power_list(2, -14, -6))\n",
    "y_params_SVM_R = mdo.AxisParams('C', bmo.power_list(2, -2, 6))\n",
    "metrics_SVM_R = outer_CV_R(n_outer_splits=5, \n",
    "                           n_inner_splits=10, \n",
    "                           X=X, \n",
    "                           y=y,\n",
    "                           axis1_params=x_params_SVM_R, \n",
    "                           axis2_params=y_params_SVM_R, \n",
    "                           train_model_callback=bmo.train_SVM_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           undersamp_ratio=0.5,\n",
    "                           oversamp_ratio=1,\n",
    "                           kernel='rbf')\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values - Uncomment for quick debugging tests  \n",
    "x_params_SVM_R = mdo.AxisParams('gamma', bmo.power_list(2, -1, 0))  \n",
    "y_params_SVM_R = mdo.AxisParams('C', bmo.power_list(2, 1, 2))  \n",
    "metrics_SVM_R = outer_CV_R(n_outer_splits=2,   \n",
    "                           n_inner_splits=2,   \n",
    "                           X=X, \n",
    "                           y=y,\n",
    "                           axis1_params=x_params_SVM_R, \n",
    "                           axis2_params=y_params_SVM_R, \n",
    "                           train_model_callback=bmo.train_SVM_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           undersample=UNDERSAMPLE,\n",
    "                           kfold_random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           kernel='rbf')  \n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec953e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for best model from each outer fold\n",
    "display(metrics_SVM_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save average of each metric\n",
    "metrics_SVM_R_mean = metrics_SVM_R.mean().to_frame().T\n",
    "R_average_metrics.loc['SVM'] = metrics_SVM_R_mean.iloc[0]\n",
    "display(metrics_SVM_R_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddd5e6",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3dd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test values\n",
    "x_params_XGB_R = mdo.AxisParams('n_estimators', [13, 25, 50, 100, 200])\n",
    "y_params_XGB_R = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16])\n",
    "metrics_XGB_R = outer_CV_R(n_outer_splits=5, \n",
    "                           n_inner_splits=10, \n",
    "                           X=X, \n",
    "                           y=y,  \n",
    "                           axis1_params=x_params_XGB_R, \n",
    "                           axis2_params=y_params_XGB_R, \n",
    "                           train_model_callback=bmo.train_XGB_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           undersample_ratio=UNDERSAMPLE,\n",
    "                           random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           objective=\"reg:squarederror\", eval_metric=\"rmse\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for quick debugging tests\n",
    "x_params_XGB_R = mdo.AxisParams('n_estimators', [1, 2])\n",
    "y_params_XGB_R = mdo.AxisParams('max_depth', [1, 2])\n",
    "metrics_XGB_R = outer_CV_R(2, 2,  \n",
    "                           X=X, \n",
    "                           y=y,  \n",
    "                           axis1_params=x_params_XGB_R, \n",
    "                           axis2_params=y_params_XGB_R, \n",
    "                           train_model_callback=bmo.train_XGB_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           undersample=UNDERSAMPLE,\n",
    "                           kfold_random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           objective=\"reg:squarederror\", eval_metric=\"rmse\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for best model from each outer fold\n",
    "display(metrics_XGB_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f701d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric and store results for analysis\n",
    "metrics_XGB_R_mean = metrics_XGB_R.mean().to_frame().T\n",
    "display(metrics_XGB_R_mean)\n",
    "R_average_metrics.loc['XGB'] = metrics_XGB_R_mean.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_R.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b59b6d",
   "metadata": {},
   "source": [
    "# Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_B(n_splits: int, X : pd.DataFrame, y_bin : pd.DataFrame, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback, kfold_random_state: int, classification_col : int, plot_title: str = \"\", \n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Perform inner cross-validation to tune model hyperparameters and find the optimal classification threshold.\n",
    "    Created: 2024/12/04\n",
    "    Parameters:\n",
    "    n_splits (int): Number of splits for KFold cross-validation.\n",
    "    X (pd.DataFrame): Feature data.\n",
    "    y (pd.DataFrame): Target data.\n",
    "    axis1_params (mdo.AxisParams): Object containing hyperparameter values for the first (horizontal) axis.\n",
    "    axis2_params (mdo.AxisParams): Object containing hyperparameter values for the second (vertical) axis.\n",
    "    train_model_callback (function): Callback function to train the model.\n",
    "    kfold_random_state (int): Random state for KFold shuffling.\n",
    "    classification_col (int): Column index to pull classification probabilities from - 0 for not top, 1 for top.\n",
    "    top_thresh_quantile (float): Threshold to classify predictions as top or not top.\n",
    "    plot_title (str, optional): Title for the ROC plot. Defaults to \"\".\n",
    "    **kwargs: Additional arguments for the train_model_callback function.\n",
    "    Returns:\n",
    "    tuple: Average best parameters (param1, param2) and the best classification threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Arrays to store parameters and binary classification thresholds of the most accurate model for each inner fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "    best_thresholds = pd.DataFrame(columns=['threshold'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_bin_train, y_bin_test = y_bin.iloc[train_index], y_bin.iloc[test_index]\n",
    "\n",
    "        # Train a grid of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_bin_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "        \n",
    "        # Use trained models to predict test set probabilities, and store in a 2D array with each cell corresponding to a model with a specific combination of parameters\n",
    "        y_proba_preds_grid = bmo.grid_predict_proba(X_test, model_grid, classification_col)\n",
    "\n",
    "        # Use probabilities to classify predictions as top or not top. This isn't the final classification, \n",
    "        # but a step towards finding the optimal threshold, so we just use the default 0.5 threshold for now.\n",
    "        y_binary_preds_grid = bmo.continuous_to_binary_absolute_grid(y_proba_preds_grid, 0.5)\n",
    "\n",
    "        # Create a 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_bin_test_grid = cdt.np_array_of_dfs(y_bin_test, y_proba_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating a 2D array of F1 scores.\n",
    "        f1_grid = bmo.calculate_f1_scores(y_binary_preds_grid, y_bin_test_grid)\n",
    "\n",
    "        # Find the index of the best F1 score in the 2D array of F1 scores\n",
    "        best_row, best_col = np.unravel_index(np.argmax(f1_grid), f1_grid.shape)\n",
    "\n",
    "        # Store hyperparameters of the most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "\n",
    "        # Find the classification probability threshold between zero and one that yields the lowest squared difference between sensitivity and \n",
    "        # specificity using this optimal model. To do this, we feed find_optimal_threshold() the probabilities predicted by the model, not the binary predictions.\n",
    "        best_model_proba_preds = y_proba_preds_grid[best_row, best_col]\n",
    "        best_thresholds.iloc[i, 0] = bmo.find_optimal_threshold_absolute(y_bin_test, best_model_proba_preds)\n",
    "\n",
    "        # Create a grid of ROC plots with predictions vs actuals, colored by F1 score for each model\n",
    "        roc_grid = plot_shaded_roc_grids(y_proba_preds_grid, y_bin_test_grid, axis1_params, axis2_params, f1_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\model_B, {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "\n",
    "        plt.show(roc_grid)\n",
    "        plt.close(roc_grid)\n",
    "\n",
    "    # Calculate average best parameters over all inner folds to return to outer CV\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    # Calculate average best threshold over all folds\n",
    "    best_threshold = best_thresholds['threshold'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fba289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_B(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "               axis1_params: mdo.AxisParams, \n",
    "               axis2_params: mdo.AxisParams, train_model_callback : callable, random_state: int, \n",
    "               classification_col : int, top_boundary_val : float, smote_preprocess = False, undersamp_ratio=1, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation with an outer and inner loop to evaluate model B performance.\n",
    "    Created: 2024/12/29\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for reproducibility in KFold.\n",
    "    classification_col : int\n",
    "        Column index to pull classification probabilities from - 0 for not top, 1 for top.\n",
    "    top_line_thresh : float\n",
    "        Threshold to classify predictions as top or not top.\n",
    "    **kwargs : dict\n",
    "        Additional parameters for the model training function.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the metrics for each outer fold, including F1 Score, Sensitivity, Specificity, and Kappa.\n",
    "    \"\"\"\n",
    "    \n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Store metrics of best model for each fold\n",
    "    kfold_metrics = pd.DataFrame(columns=['F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        y_train_bin = bmo.continuous_to_binary_absolute(y_train, top_boundary_val)\n",
    "        y_test_bin = bmo.continuous_to_binary_absolute(y_test, top_boundary_val)\n",
    "\n",
    "        if smote_preprocess:\n",
    "            sm = SMOTE(random_state=random_state)\n",
    "            X_train, y_train_bin = sm.fit_resample(X_train, y_train_bin)\n",
    "\n",
    "            # Split the top and not top lines into seperate dataframes\n",
    "            y_train_top = y_train_bin[y_train_bin == 1]\n",
    "            X_train_top = X_train.loc[y_train_top.index]\n",
    "            y_train_not_top = y_train_bin[y_train_bin == 0]\n",
    "            X_train_not_top = X_train.loc[y_train_not_top.index]\n",
    "\n",
    "            # Undersample the majority class so that it is a certain proportion of its original size\n",
    "            X_train_not_top_us, y_train_not_top_us = cdt.random_subset(X_train_not_top, y_train_not_top, p=undersamp_ratio, random_state=random_state)\n",
    "\n",
    "            # Re-combine the top and not top line data\n",
    "            X_train = pd.concat([X_train_top, X_train_not_top_us], axis=0)\n",
    "            y_train = pd.concat([y_train_top, y_train_not_top_us], axis=0)\n",
    "        \n",
    "        X_train, _, X_scaler, _ = scale_features_and_target(X_train, None)\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "\n",
    "        # Find average best parameters and threshold based on F1 score using inner-fold CV\n",
    "        best_param1, best_param2, best_threshold = inner_CV_B(n_inner_splits, X_train, y_train_bin, axis1_params, axis2_params, train_model_callback, random_state, classification_col, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train, np.ravel(y_train_bin), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set probabilities\n",
    "        y_pred_proba = pd.DataFrame(super_model.predict_proba(X_test)[:, classification_col], index=y_test_bin.index, columns=y_test_bin.columns)\n",
    "        histogram(y_pred_proba, f'Top Line Probability Histogram, {train_model_callback.__name__}, Outer Fold {i}', x_ax_label='Top Line Probability')\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean) using the best threshold as determined by inner CV\n",
    "        y_pred_bin = bmo.continuous_to_binary_absolute(y_pred_proba, best_threshold)\n",
    "\n",
    "        # Plot ROC and PR curves using seaborn\n",
    "        cmp.sns_plot_roc_curve(pd.DataFrame(y_test_bin), pd.DataFrame(y_pred_proba), f'Model B ROC Curve, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "        plt.savefig(f'{storage_dir}\\\\Model B ROC Curve, {train_model_callback.__name__}, Outer Fold {i}.svg', format=\"svg\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        cmp.sns_plot_pr_curve(pd.DataFrame(y_test_bin), pd.DataFrame(y_pred_proba), f'Model B PR Curve, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "        plt.savefig(f'{storage_dir}\\\\Model B PR Curve, {train_model_callback.__name__}, Outer Fold {i}.svg', format=\"svg\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "        # Calculate classification metrics and add new row to kfold_metrics   \n",
    "        classification_metrics = cdt.classification_metrics(y_pred_bin, y_test_bin)   \n",
    "        kfold_metrics = pd.concat([kfold_metrics, classification_metrics], axis=0)   \n",
    "\n",
    "    # Label each row of kfold_metrics with the fold number\n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    return kfold_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7457c6",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter grids for SVM classification model and conduct 2-dimensional cross-validation\n",
    "x_params_SVM_B = mdo.AxisParams('gamma', bmo.power_list(2, -18, -8))\n",
    "y_params_SVM_B = mdo.AxisParams('C', bmo.power_list(2, 2, 16))\n",
    "metrics_SVM_B = outer_CV_B(5, 10, X, y, x_params_SVM_B, y_params_SVM_B, bmo.train_SVM_classifier, \n",
    "                           random_state=RANDOM_STATE, classification_col=1, top_boundary_val=top_boundary_val, \n",
    "                           smote_preprocess=SMOGN_PREPROCESS, probability=True, kernel='rbf')\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for tests\n",
    "x_params_SVM_B = mdo.AxisParams('gamma', bmo.power_list(2, -10, -9))\n",
    "y_params_SVM_B = mdo.AxisParams('C', bmo.power_list(2, 0, 1))\n",
    "metrics_SVM_B = outer_CV_B(2, 2, X, y, x_params_SVM_B, y_params_SVM_B, bmo.train_SVM_classifier, kfold_random_state=RANDOM_STATE, kernel='rbf', classification_col=1, top_boundary_val=top_boundary_val, probability=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7219ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for super-model trained on all data from each outer fold\n",
    "display(metrics_SVM_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_SVM_B_mean = metrics_SVM_B.mean().to_frame().T\n",
    "B_average_metrics.loc['SVM'] = metrics_SVM_B_mean.iloc[0]\n",
    "display(metrics_SVM_B_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf6cd8",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for XGB classification model and conduct 2-dimensional cross-validation\n",
    "x_params_XGB_B = mdo.AxisParams('n_estimators', [3, 7, 13, 25, 50, 100, 200, 400])\n",
    "y_params_XGB_B = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16, 32, 64])\n",
    "metrics_XGB_B = outer_CV_B(5, 10, X, y, x_params_XGB_B, y_params_XGB_B, bmo.train_XGB_classifier, random_state=RANDOM_STATE, \n",
    "                           classification_col=1, top_boundary_val=top_boundary_val, smote_preprocess=SMOGN_PREPROCESS, \n",
    "                           objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for quick tests\n",
    "x_params_XGB_B = mdo.AxisParams('n_estimators', [1, 2])\n",
    "y_params_XGB_B = mdo.AxisParams('max_depth', [1, 2])\n",
    "metrics_XGB_B = outer_CV_B(2, 2, X, y, x_params_XGB_B, y_params_XGB_B, bmo.train_XGB_classifier, random_state=RANDOM_STATE, \n",
    "                           classification_col=1, top_boundary_val=top_boundary_val, smote_preprocess=SMOGN_PREPROCESS, \n",
    "                           objective=\"binary:logistic\", eval_metric=\"logloss\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_XGB_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_XGB_B_mean = metrics_XGB_B.mean().to_frame().T\n",
    "B_average_metrics.loc['XGB'] = metrics_XGB_B_mean.iloc[0]\n",
    "display(metrics_XGB_B_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_B.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de75da",
   "metadata": {},
   "source": [
    "# Model RO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82681b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_RO(n_splits: int, X : pd.DataFrame, y : pd.DataFrame, y_top_bound : float, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, train_model_callback, \n",
    "                kfold_random_state: int, plot_title: str = \"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Perform inner cross-validation (RO) to find the best model parameters and classification threshold.\n",
    "    Created: 2024/12/21\n",
    "    Parameters:\n",
    "    n_splits (int): Number of splits for K-Fold cross-validation.\n",
    "    X (pd.DataFrame): Feature data.\n",
    "    y (pd.DataFrame): Target data.\n",
    "    axis1_params (mdo.AxisParams): Hyperparameters to explore for the horizontal axis.\n",
    "    axis2_params (mdo.AxisParams): Hyperparameters to explore for the vertical axis.\n",
    "    train_model_callback (callable): Callback function to train the model.\n",
    "    kfold_random_state (int): Random state for K-Fold shuffling.\n",
    "    top_line_threshold (float): Threshold to classify top values during intermediate step in inner CV.\n",
    "    plot_title (str, optional): Title for the plot. Defaults to \"\".\n",
    "    **kwargs: Additional keyword arguments for the model training callback.\n",
    "    Returns:\n",
    "    tuple: Average best parameters for axis1 and axis2, and the best classification threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create KFold object for inner-fold cross-validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Store best parameters (param1, param2) for each fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "    best_thresholds = pd.DataFrame(columns=['threshold'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train a grid of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "\n",
    "        # Use trained models to predict test set labels, and store in a 2D array with each cell corresponding to a model with a specific combination of parameters\n",
    "        y_preds_grid = bmo.grid_predict(X_test, model_grid)\n",
    "\n",
    "        # Create a 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_test_grid = cdt.np_array_of_dfs(y_test, y_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating a 2D array of Pearson coefficients\n",
    "        pearson_grid = bmo.calculate_pearson_coefficients(y_preds_grid, y_test_grid)\n",
    "\n",
    "        # Find the index of the best Pearson coefficient in the 2D array of Pearson coefficients\n",
    "        best_row, best_col = np.unravel_index(np.argmax(pearson_grid), pearson_grid.shape)\n",
    "        \n",
    "        # Store hyperparameters of the most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "\n",
    "        # Extract best model's continuous predictions\n",
    "        best_model_y_preds = y_preds_grid[best_row, best_col]\n",
    "\n",
    "        # Classify labels as top or not top (boolean)\n",
    "        y_test_binary = bmo.continuous_to_binary_absolute(y_test, y_top_bound)\n",
    "        \n",
    "        # Find classification threshold that yields the lowest squared difference between sensitivity and specificity using this optimal model\n",
    "        best_absolute_thresh = bmo.find_optimal_threshold_absolute(y_test_binary, best_model_y_preds)\n",
    "        best_thresholds.iloc[i, 0] = best_absolute_thresh\n",
    "\n",
    "        # Create a grid of scatter plots with predictions vs actuals, colored by Pearson coefficient for each model\n",
    "        scatter_grid = plot_shaded_scatter_grids(y_preds_grid, y_test_grid, axis1_params, axis2_params, pearson_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\model_RO, {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "        plt.show(scatter_grid)\n",
    "        plt.close(scatter_grid)\n",
    "\n",
    "    # Calculate average best parameters over all folds\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    # Calculate average best threshold over all folds\n",
    "    best_threshold = best_thresholds['threshold'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_RO(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "                axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, train_model_callback : callable, \n",
    "                kfold_random_state: int, top_boundary_val : float, smogn_preprocess : bool = False, undersample : bool = True,\n",
    "                **kwargs) -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Perform outer cross-validation with nested inner cross-validation for model RO selection and evaluation.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Callback function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for reproducibility in KFold splitting.\n",
    "    top_line_threshold : float\n",
    "        Threshold for classifying top predictions during intermediate step in inner CV.\n",
    "    **kwargs : dict\n",
    "        Additional parameters for the model training callback.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the evaluation metrics for each outer fold, including Pearson correlation, F1 Score, Sensitivity, Specificity, and Kappa.\n",
    "    list\n",
    "        List of trained super_models for each outer fold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create KFold object for outer loop to split data into train and test sets\n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Initialize DataFrame to store evaluation metrics for each outer fold\n",
    "    kfold_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "    \n",
    "    # Initialize list to store super_models for each outer fold\n",
    "    super_models = []\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if smogn_preprocess:\n",
    "            top_boundary_quantile_in_train_set = percentileofscore(y_train.to_numpy().flatten(), top_boundary_val, kind='mean') / 100\n",
    "\n",
    "            # Store pre-SMOGN data for later use\n",
    "            X_train_pre_smogn = X_train.copy()\n",
    "            y_train_pre_smogn = y_train.copy()\n",
    "            X_train, y_train = smogn_prep(X_train, y_train, top_boundary_quantile_in_train_set, undersample)\n",
    "\n",
    "            if not undersample:\n",
    "                # Manually concatenate the original data below the augmentation threshold with the augmented data\n",
    "                non_augmented_indices = y_train_pre_smogn[y_train_pre_smogn < top_boundary_val].index\n",
    "                X_train_non_augmented = X_train_pre_smogn.loc[non_augmented_indices]\n",
    "                y_train_non_augmented = y_train_pre_smogn.loc[non_augmented_indices]\n",
    "                X_train = pd.concat([X_train, X_train_non_augmented], axis=0)\n",
    "                y_train = pd.concat([y_train, y_train_non_augmented], axis=0)\n",
    "\n",
    "            # Split the augmented and original data into seperate columns for plotting\n",
    "            original_data = pra.intersection(y_train_pre_smogn, y_train)\n",
    "            augmented_data = pra.difference(y_train, original_data)\n",
    "            orig_aug_data = pd.concat([original_data, augmented_data], axis=1)\n",
    "            orig_aug_data.columns = ['Original GY', 'Augmented GY']\n",
    "\n",
    "            histogram(orig_aug_data, f'Model RO SMOGN-Augmented GY Histogram, Outer Fold {i}', vline_value=top_boundary_val)\n",
    "        else:\n",
    "            histogram(y_train, f'Model RO Histogram, Outer Fold {i}', vline_value=top_boundary_val)\n",
    "\n",
    "\n",
    "        # Scale features and target for the training data\n",
    "        X_train, y_train, X_scaler, y_scaler = scale_features_and_target(X_train, y_train)\n",
    "        # Scale the top boundary value\n",
    "        top_boundary_val_scaled = y_scaler.transform([[top_boundary_val]])[0, 0]\n",
    "        # Scale the test data using the same scaler as the training data\n",
    "        X_test = pd.DataFrame(X_scaler.transform(X_test))\n",
    "        X_test.columns = X.columns\n",
    "        y_test = pd.DataFrame(y_scaler.transform(y_test))\n",
    "\n",
    "        # Find average best parameters and threshold based on Pearson score using inner-fold CV\n",
    "        best_param1, best_param2, best_threshold_fixed = inner_CV_RO(n_inner_splits, X_train, y_train, top_boundary_val_scaled, axis1_params, axis2_params, train_model_callback, kfold_random_state, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train, np.ravel(y_train), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "        \n",
    "        # Append the trained super_model to the list\n",
    "        super_models.append(super_model)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set\n",
    "        y_pred = pd.DataFrame(super_model.predict(X_test).reshape(-1, 1), index=y_test.index, columns=y_test.columns)\n",
    "        # Plot histogram of the predicted values\n",
    "        histogram(y_pred, f'Predicted GY Histogram, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "\n",
    "        # Calculate Pearson coefficient of continuous predictions\n",
    "        pearson, _ = pearsonr(np.ravel(y_pred), np.ravel(y_test))\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean)\n",
    "        y_pred_top = bmo.continuous_to_binary_absolute(y_pred, best_threshold_fixed)\n",
    "        y_test_top = bmo.continuous_to_binary_absolute(y_test, top_boundary_val_scaled)\n",
    "\n",
    "        # Plot classification results\n",
    "        cmp.plot_classification_results(y_pred, y_test, y_pred_top, y_test_top, \n",
    "                                [f\"Model RO Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}\"],\n",
    "                                save_path=f'{storage_dir}\\\\Model RO Super Model Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}.svg')\n",
    "\n",
    "        # Calculate classification metrics and add new row to kfold_metrics\n",
    "        classification_metrics = cdt.classification_metrics(y_pred_top, y_test_top)\n",
    "        pearson_df = pd.DataFrame([pearson], columns=['Pearson'])\n",
    "        metrics_row = pd.concat([pearson_df, classification_metrics], axis=1)\n",
    "        kfold_metrics = pd.concat([kfold_metrics, metrics_row], axis=0)\n",
    "    \n",
    "    # Reset index of the metrics DataFrame\n",
    "\n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    return kfold_metrics, super_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac635306",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set hyperparameter grids for SVM regression model and conduct 2-dimensional cross-validation\n",
    "x_params_SVM_RO = mdo.AxisParams('gamma', bmo.power_list(2, -14, -6))\n",
    "y_params_SVM_RO = mdo.AxisParams('C', bmo.power_list(2, -2, 6))\n",
    "metrics_SVM_RO, RO_SVM_models = outer_CV_RO(5, 10, X, y, x_params_SVM_RO, y_params_SVM_RO, bmo.train_SVM_regressor, \n",
    "                             kfold_random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, smogn_preprocess=SMOGN_PREPROCESS, \n",
    "                            undersample=UNDERSAMPLE, kernel='rbf')\n",
    "\n",
    "\"\"\"\n",
    "# Quick test values\n",
    "x_params_SVM_RO = mdo.AxisParams('gamma', bmo.power_list(2, -8, -7))\n",
    "y_params_SVM_RO = mdo.AxisParams('C', bmo.power_list(2, 0, 1))\n",
    "metrics_SVM_RO, RO_SVM_models = outer_CV_RO(2, 2, X, y, x_params_SVM_RO, y_params_SVM_RO, bmo.train_SVM_regressor, \n",
    "                             kfold_random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, smogn_preprocess=SMOGN_PREPROCESS, \n",
    "                             undersample=UNDERSAMPLE, kernel='rbf')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP:\n",
    "    # Create a background dataset (using ~25% of data)\n",
    "    n_background = 200  # 25% of 800 points\n",
    "    background_data = shap.sample(X, n_background, random_state=42)  # Using random sampling for better representation\n",
    "\n",
    "    # Create the SHAP explainer with the correct parameters\n",
    "    explainer = shap.KernelExplainer(RO_SVM_models[0].predict, background_data)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Create the summary plot\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(f'{storage_dir}\\\\SHAP Summary Plot, SVM, Outer Fold 0.svg', format=\"svg\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975314ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_SVM_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb307e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_SVM_RO_mean = metrics_SVM_RO.mean().to_frame().T\n",
    "RO_average_metrics.loc['SVM'] = metrics_SVM_RO_mean.iloc[0]\n",
    "display(metrics_SVM_RO_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2313db",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for XGB regression model and conduct 2-dimensional cross-validation\n",
    "x_params_XGB_RO = mdo.AxisParams('n_estimators', [3, 7, 13, 25, 50, 100, 200])\n",
    "y_params_XGB_RO = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16, 32, 64])\n",
    "metrics_XGB_RO, RO_XGB_models = outer_CV_RO(5, 10, X, y, x_params_XGB_RO, y_params_XGB_RO, bmo.train_XGB_regressor, \n",
    "                             kfold_random_state=RANDOM_STATE, random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, \n",
    "                             smogn_preprocess=SMOGN_PREPROCESS, undersample=UNDERSAMPLE, objective=\"reg:squarederror\", \n",
    "                             eval_metric=\"rmse\")\n",
    "\"\"\"\n",
    "# Quick test values\n",
    "x_params_XGB_RO = mdo.AxisParams('n_estimators', [1, 2])\n",
    "y_params_XGB_RO = mdo.AxisParams('max_depth', [1, 2])\n",
    "metrics_XGB_RO, RO_XGB_models = outer_CV_RO(2, 2, X, y, x_params_XGB_RO, y_params_XGB_RO, bmo.train_XGB_regressor, \n",
    "                                kfold_random_state=RANDOM_STATE, random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, \n",
    "                                smogn_preprocess=SMOGN_PREPROCESS, undersample=UNDERSAMPLE, objective=\"reg:squarederror\", \n",
    "                                eval_metric=\"rmse\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3774b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP:\n",
    "    # Create a background dataset (using ~25% of data)\n",
    "    n_background = 200  # 25% of 800 points\n",
    "    background_data = shap.sample(X, n_background, random_state=42)\n",
    "\n",
    "    # Create a wrapper function for the model's predict method\n",
    "    def model_predict(x):\n",
    "        return RO_XGB_models[0].predict(x)\n",
    "\n",
    "    # Create the SHAP explainer with the wrapper function\n",
    "    explainer = shap.KernelExplainer(model_predict, background_data)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Create the summary plot\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(f'{storage_dir}\\\\SHAP Summary Plot, SVM, Outer Fold 0.svg', format=\"svg\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37545449",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_XGB_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521925c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_XGB_RO_mean = metrics_XGB_RO.mean().to_frame().T\n",
    "RO_average_metrics.loc['XGB'] = metrics_XGB_RO_mean.iloc[0]\n",
    "display(metrics_XGB_RO_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_RO.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdc2c5",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average metrics for each model to compare GBLUP, SVM, and XGB\n",
    "R_avg_metrics_plot = cmp.plot_model_metrics(R_average_metrics, \"R\")\n",
    "R_avg_metrics_plot.savefig(f'{storage_dir}\\\\R_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(R_avg_metrics_plot)\n",
    "plt.close(R_avg_metrics_plot)\n",
    "\n",
    "B_avg_metrics_plot = cmp.plot_model_metrics(B_average_metrics, \"B\")\n",
    "B_avg_metrics_plot.savefig(f'{storage_dir}\\\\B_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(B_avg_metrics_plot)\n",
    "plt.close(B_avg_metrics_plot)\n",
    "\n",
    "RO_avg_metrics_plot = cmp.plot_model_metrics(RO_average_metrics, \"RO\")\n",
    "RO_avg_metrics_plot.savefig(f'{storage_dir}\\\\RO_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(RO_avg_metrics_plot)\n",
    "plt.close(RO_avg_metrics_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
