{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5e364c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2c054d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dill\n",
    "# %pip install pyreadr\n",
    "# %pip install scikit-learn\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install xgboost\n",
    "# %pip install -U matplotlib\n",
    "# %pip install -U seaborn\n",
    "# %pip install typeguard\n",
    "# %pip install PyQt6\n",
    "# %pip install smogn\n",
    "# %pip install seaborn\n",
    "# %pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40a6cbc7-e77d-47fa-b099-e0f911c53168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library to check function types of imported modules\n",
    "from typeguard import install_import_hook\n",
    "\n",
    "# custom functions for plotting, etc.\n",
    "with install_import_hook('custom_ml_plots'):\n",
    "    import custom_ml_plots as cmp\n",
    "with install_import_hook('custom_dataset_tools'):\n",
    "    import custom_dataset_tools as cdt\n",
    "with install_import_hook('basic_ml_operations'):\n",
    "    import basic_ml_operations as bmo\n",
    "with install_import_hook('ml_data_objects'):\n",
    "    import ml_data_objects as mdo\n",
    "\n",
    "# data import and export\n",
    "import pyreadr\n",
    "import dill\n",
    "\n",
    "# data management libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# plots\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# misc\n",
    "import os\n",
    "\n",
    "# preprocessing\n",
    "from smogn import smoter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# k-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# global parameters\n",
    "RANDOM_STATE = 42\n",
    "TOP_THRESHOLD_QUANTILE = 0.8 # values to test: 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "SMOGN_PREPROCESS = True\n",
    "UNDERSAMPLE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93a884",
   "metadata": {},
   "source": [
    "## Setup Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e4180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_shaded_scatter_grids(y_preds_grid: np.ndarray, y_test_grid: np.ndarray, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, pearson_grid: np.ndarray, plot_title: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot predictions vs actuals and colour by pearson coefficient and add best fit\n",
    "    Created: 2024/11/30\n",
    "    \"\"\"\n",
    "    # create plot of predictions vs actuals\n",
    "    fig, axs = cmp.create_scatter_grid(y_preds_grid, y_test_grid, axis1_params, axis2_params, plot_title)\n",
    "\n",
    "    # colour by pearson coefficient and add best fit and title\n",
    "    cmp.color_spectrum(fig, axs, pearson_grid, label=\"Pearson Coefficient\")\n",
    "    cmp.add_best_fit(axs)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85b3abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_shaded_roc_grids(y_preds_grid: np.ndarray, y_test_grid: np.ndarray, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, f1_grid: np.ndarray, plot_title: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot predictions vs actuals and colour by pearson coefficient and add best fit\n",
    "    Created: 2024/12/22\n",
    "    \"\"\"\n",
    "    # create plot of predictions vs actuals\n",
    "    fig, axs = cmp.create_roc_grid(y_preds_grid, y_test_grid, axis1_params, axis2_params, plot_title)\n",
    "\n",
    "    # colour by pearson coefficient and add best fit and title\n",
    "    cmp.color_spectrum(fig, axs, f1_grid, label=\"f1 Score\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e13ebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_average_metrics = pd.DataFrame(columns=['F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "R_average_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "RO_average_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "# Add linear regression model metrics from research paper\n",
    "B_average_metrics.loc['GBLUP'] =  [0.411, 0.696, 0.577, 0.180]\n",
    "R_average_metrics.loc['GBLUP'] =  [None, 0.215, 0.128, 0.987, 0.164]\n",
    "RO_average_metrics.loc['GBLUP'] = [None, 0.487, 0.711, 0.699, 0.304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "740e548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_GY_hist(GY_df: pd.DataFrame, title: str, x_ax_label: str = 'Grain Yield (GY)', y_ax_label: str = 'Frequency') -> None:\n",
    "    \"\"\"\n",
    "    Create a histogram of grain yield values\n",
    "    Created: 2024/01/12\n",
    "\n",
    "    Args:\n",
    "        GY_df (pd.DataFrame): DataFrame containing grain yield values\n",
    "        title (str): Title of the plot\n",
    "    \"\"\"\n",
    "    # Create histogram\n",
    "    plt.hist(GY_df, bins=60, edgecolor='black')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_ax_label)\n",
    "    plt.ylabel(y_ax_label)\n",
    "    plt.title(title)\n",
    "    plt.savefig(f'{storage_dir}\\\\{title}.svg', format=\"svg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3df08d",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a24d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numbered_subdir():\n",
    "    \"\"\"\n",
    "    Creates a new subdirectory within the 'saved_data_and_plots' directory, \n",
    "    with a name that is the next available number in sequence, formatted as a \n",
    "    three-digit number (e.g., '001', '002', etc.).\n",
    "    Created: 2024/01/01\n",
    "    Returns:\n",
    "        str: The path to the newly created numbered subdirectory.\n",
    "    \"\"\"\n",
    "    # Create parent directory if it doesn't exist\n",
    "    parent_dir = \"saved_data_and_plots\"\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    \n",
    "    # Find next available number\n",
    "    existing_dirs = [d for d in os.listdir(parent_dir) \n",
    "                    if os.path.isdir(os.path.join(parent_dir, d))]\n",
    "    existing_nums = [int(d) for d in existing_dirs if d.isdigit()]\n",
    "    next_num = max(existing_nums + [-1]) + 1\n",
    "    \n",
    "    # Create new numbered directory\n",
    "    new_dir = os.path.join(parent_dir, f\"{next_num:03d}\")\n",
    "    os.makedirs(new_dir)\n",
    "    return new_dir\n",
    "\n",
    "storage_dir = create_numbered_subdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e06ae33-23b5-42a2-a4c1-e063428c27b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>6.119272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>5.905515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>2.160587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>6.456711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688880</th>\n",
       "      <td>3.616688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GY\n",
       "GID                 \n",
       "GID6569128  6.119272\n",
       "GID6569128  5.905515\n",
       "GID6569128  2.160587\n",
       "GID6569128  6.456711\n",
       "GID6688880  3.616688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAHJCAYAAACWmnNkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPstJREFUeJzt3Xl8U1X+//F3SlvaShXaQossVopU9r1sIsuoIC7DooJaFYShKIICCiggmwgMCAiIDIsyDKIoMKAOiwvi+FNAyuY4lp1SQLbQFoEmXWh+f/htxtB0S5ekN6/n48EDcu65uZ/kNumbm3NOTDabzSYAAAAD8HF3AQAAACWFYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMgl2PHjmnq1Knq1q2bmjZtqpYtW6pfv35avXq1srKyJEk2m01PP/20GjVqpMOHDzu9n48++kjR0dH68MMPCzzmU089paeeeirP7V27dtXYsWNde0ClLDo6WgsWLCj2/SxYsEDR0dElUBHgvQg2ABxs2rRJvXv31r59+zRgwAAtWbJEc+bMUYMGDfTmm29q2LBhstlsMplMmjZtmvz8/DR+/HhlZ2c73M+5c+c0a9YsdezYUY8//ribHg0Ab+Pr7gIAeI5jx47p1VdfVceOHTVv3jz5+v7vLaJTp05q06aNhg8frs2bN6tHjx6qVauWRo4cqTfeeEMrV65U//797f0nTpwoX19fTZs2zQ2PBIC34ooNALtly5bJx8dHkydPdgg1Obp166aePXs6tMXGxqpVq1Z6++23debMGUnS559/ru3bt+v1119XeHh4qdTatWtXzZ8/XzNnzlT79u3VpEkTDRw4UImJifY+ycnJGjVqlDp06KDGjRvrz3/+szZs2OBwP8ePH9cLL7ygmJgYtW7dWnFxcTp27Jh9++nTpzV69Gjdddddatiwodq1a6fRo0crJSUlz9pSU1P1+uuvq3379mrcuLEee+wx7dixw6FPenq6pk+frg4dOqh58+Z69dVXlZ6eXiLPDeDNCDYA7L7++mu1bdtWoaGhefaZOXOmevToYb9tMpn05ptvKjs7WzNmzNCVK1c0Y8YM3X///XrggQdKtd6VK1fq+PHjmj59ut544w39/PPPGjNmjH37K6+8omPHjmny5MlaunSpGjRooDFjxmjnzp2SpPPnz6tv375KTEzUpEmTNGvWLJnNZj3zzDNKTU2VxWLR008/rWPHjmnixIlavny5nn76af3rX//S3LlzndaUnp6uZ555Rl9//bVGjBihhQsXKiIiQoMGDXIIN6+88oo+/vhjxcXFad68ebp8+bJWrFhRqs8X4A34KAqAJOny5cu6fPmyIiMjc23LGTCcw2QyqUKFCvbbt912m0aMGKHp06fbr2RMmjSpNMuVJN18881atGiRvZakpCQtWLBAKSkpqlKlin788UcNHTpU99xzjyQpJiZGlStXlr+/vyRpxYoVysjI0Pvvv6+qVatKku688049/vjjOnDggKpVq6aIiAjNnDlTtWrVkiS1bdtWBw4c0I8//ui0po0bN+rgwYP6+OOP1bRpU0nS3XffraeeekqzZ8/WunXrdOTIEW3dulWTJk2yjz/q2LGjHnroIR09erT0njDACxBsAEhSrsG/OU6ePKn77rvPoa1GjRratm2bQ9vTTz+tLVu2aPfu3VqyZIkqV65c4jWaTCaH240bN3YIWBEREZIki8WiKlWqqE2bNlqwYIF++eUXdezYUZ06dXK4orNnzx41a9bMHmpy7uObb76x3169erWys7OVmJiokydP6ujRozp+/HiusJdjx44dqlq1qho2bOjQp0uXLvrrX/+qy5cvKz4+XtLvH6fl8PHxUbdu3Qg2QDERbABIkqpUqaKgoCD7OJkc1atX19q1a+2333nnHafTu318fNShQwft27dPnTp1KvLxg4KClJqamuf2jIwMBQYGOrTdeNvH5/dP13NC2ty5c7V48WJt3rxZW7dulY+Pj9q3b68pU6aoRo0aSk1NVc2aNfOt6/3339fixYuVmpqqsLAwNWrUSIGBgbpy5YrT/qmpqbp48aIaNmzodPvFixd1+fJlSb8/53/0x4AFwDUEGwB2Xbt21TfffKOrV6+qUqVKkiR/f381btzY3qc0rsRIUlhYWJ7r4WRkZCg5OVlhYWFFus/g4GC98soreuWVV3T8+HF9/fXXWrRokSZPnqwlS5YoODhYycnJufbbsWOHatasqf3792vGjBl65ZVX1Lt3b4WEhEiSXnzxRf3nP//J85iRkZGaPXu20+01a9a0Bxqz2axbb73Vvi2/YAegcBg8DMBu8ODBysrK0vjx45WRkZFru9Vq1alTp0rl2DExMfr111+1f//+XNu++uorXb9+XW3bti30/Z05c0adOnXSli1bJEl16tTRX/7yF7Vv316//vqrJKlVq1Y6cOCAQ7i5dOmSBg0apG+//VZ79uzRzTffrEGDBtlDzbVr17Rnz548P7qLiYnR2bNnFRoaqsaNG9v/fP/991q2bJkqVKhgfxw5teX440dgAFzDFRsAdtHR0Zo1a5ZeffVV9e7dW4888oiio6OVlZWlffv2ae3atTKbzRo0aFCR7vfo0aPKyMhQgwYN8uzTo0cP/f3vf1dcXJzi4uLUsGFDZWdna+/evVq2bJkefPBBtWjRotDHrFGjhiIiIvTGG2/o6tWrql27tn7++Wd9++23iouLkyT1799fGzZs0KBBgxQXFyc/Pz+9++67ioiI0EMPPaSvv/5aH374oWbMmKEuXbrowoULWr58ucxms2655Ranx+3du7dWrVqlAQMGaMiQIapevbp++OEHLV26VLGxsfLz89Ntt92mvn37au7cucrKylL9+vW1ceNGHTp0qEjPK4DcCDYAHHTr1k2NGjXShx9+qLVr1+rMmTOy2WyqVauWevTooX79+jmdOZWfyZMn68yZM7kGHP+Rn5+fVq1apcWLF+uTTz7R/Pnz5ePjY59xFRsbW+THsnDhQs2ZM0dvv/22UlJSVL16db3wwgsaPHiwpN/HD61evVqzZs3S2LFj5e/vrzZt2mju3Lm65ZZb1KtXL50+fVrr1q3T6tWrFR4erk6dOumJJ57QhAkTdOzYMUVFRTkcMygoSB988IHeeustzZo1S1euXFGNGjU0atQoPfvss/Z+EydOVFhYmFatWqXLly+rY8eOGjJkiObNm1fkxwngf0w2m83m7iIAGFtGRoZ69+6tzz//3N2lADA4xtgAKHXLli1TmzZt3F0GAC/AFRsApe7QoUOKiopy+jUNAFCSCDYAAMAw+CgKAAAYBsEGAAAYBsEGAAAYhteN5Nu3b59sNpv8/PzcXQoAACikzMxMmUwmNW/ePN9+XnfFxmazyZvHS9tsNmVkZHj1c1CecL7KF85X+cL5Kl8K+/vb667Y5Fyp+eOX+nmTtLQ0JSQkqG7dugoKCnJ3OSgA56t84XyVL5yv8iWvL569kdddsQEAAMZFsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIZBsAEAAIbh6+4CAMAIkpKSdOrUKSUmJspqtSowMNC+LSwsTLVr13ZjdYD3INgAQDElJSUp+s76slrSnG4PCAzSoYMJhBugDBBsAKCYzGazrJY0Nbt/hIJDajpsu5J8Wvs3z5XZbCbYAGWAYAMAJSQ4pKZuCY9ydxmAV2PwMAAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyCDQAAMAyPCjYnTpxQ8+bNtX79entbQkKCYmNj1axZM3Xt2lUrV650Y4UAAMCTeUywyczM1Msvv6y0tDR7W0pKigYMGKDatWtr3bp1Gjp0qGbPnq1169a5sVIAAOCpfN1dQI4FCxaoUqVKDm0ff/yx/Pz8NGXKFPn6+ioqKkonT57UkiVL1KdPHzdVCgAAPJVHXLHZvXu31qxZoxkzZji0x8fHKyYmRr6+/8tfbdu2VWJiosxmc1mXCQAAPJzbr9j89ttvGj16tMaPH6/q1as7bDt37pzq1avn0FatWjVJ0tmzZxUWFubSMW02m8NHXt7EYrE4/A3PxvkqHwpzfiwWi9e+73gqXl/li81mk8lkKrCf24PNpEmT1Lx5cz300EO5tlmtVvn7+zu0VaxYUZKUnp7u8jEzMzOVkJDg8v5GkJiY6O4SUAScL89WmPOTmJiogICA0i8GRcbrq/y4MRM449Zgs2HDBsXHx+uzzz5zuj0gIEAZGRkObTmBJigoyOXj+vn5qW7dui7vX55ZLBYlJiYqMjJSgYGB7i4HBeB8lQ9Wq7XAPpGRkapfv34ZVIPC4vVVvhw9erRQ/dwabNatW6dLly6pc+fODu0TJ07Upk2bFBERoQsXLjhsy7kdHh7u8nFNJlOxgpERBAYGev1zUJ5wvjxbYX4pcg49F+emfCjMx1CSm4PN7Nmzc/1P57777tPw4cP18MMPa+PGjfroo490/fp1VahQQZK0c+dO3X777QoNDXVHyQAAwIO5dVZUeHi4brvtNoc/khQaGqrw8HD16dNHV69e1bhx43T06FGtX79eK1asUFxcnDvLBgAAHsojpnvnJTQ0VMuWLdOJEyfUq1cvLVy4UKNHj1avXr3cXRoAAPBAbp8VdaNDhw453G7SpInWrFnjpmoAAEB54tFXbAAAAIqCYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAyDYAMAAAzD49axAYDSkpSUJLPZnOf2sLAw1a5duwwrAlDSCDYAvEJSUpKi76wvqyUtzz4BgUE6dDCBcAOUYwQbAF7BbDbLaklTs/tHKDikZq7tV5JPa//muTKbzQQboBwj2ADwKsEhNXVLeJS7ywBQShg8DAAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADMPX3QUAgLdLSkqS2Wx2ui0sLEy1a9cu44qA8otgAwBulJSUpOg768tqSXO6PSAwSIcOJhBugEIi2ACAG5nNZlktaWp2/wgFh9R02HYl+bT2b54rs9lMsAEKiWADAB4gOKSmbgmPcncZQLnH4GEAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYBBsAAGAYfAkmABRCUlKSzGaz020JCQllXA2AvBBsAKAASUlJir6zvqyWNHeXAqAABBsAKIDZbJbVkqZm949QcEjNXNvPn9ijwz+sdkNlAG5EsAGAQgoOqalbwqNytV9NPu2GagA4w+BhAABgGAQbAABgGAQbAABgGIyxAYA/cDZ1m+ncQPlBsAEASdZrKZLJpNjYWHeXAqAYCDYAICkr/Zpkszmd0s10bqD8INgAwB84m9LNdG6g/GDwMAAAMAyCDQAAMAyCDQAAMAyCDQAAMAwGDwOAl0pKSpLZbHa6LSwsTLVr1y7jioDiI9gAgBdKSkpS9J31ZbWkOd0eEBikQwcTCDcodwg2AOCFzGazrJY0p+v2XEk+rf2b58psNhNsUO4QbADAizlbtwcoz9w+ePjSpUt65ZVX1LZtWzVv3lyDBw/WsWPH7NsTEhIUGxurZs2aqWvXrlq5cqUbqwUAAJ7M7cFm6NChOnnypJYsWaK1a9cqICBA/fv3l8ViUUpKigYMGKDatWtr3bp1Gjp0qGbPnq1169a5u2wAAOCB3PpR1OXLl1WjRg3FxcWpXr16kqTnn39ef/7zn3XkyBHt2LFDfn5+mjJlinx9fRUVFWUPQX369HFn6QAAwAO59YrNLbfcorfeesseapKTk7VixQpFRESobt26io+PV0xMjHx9/5e/2rZtq8TExDynKAIAAO/lMYOHJ0yYoI8//lj+/v569913FRQUpHPnztlDT45q1apJks6ePauwsDCXjmWz2ZSW5nyKo9FZLBaHv+HZOF8lx93PocVicfq+U5i69u/fn2e/sLAw1apVy6V6CtPHyO+VvL7KF5vNJpPJVGA/jwk2zzzzjPr27asPPvhAQ4cO1erVq2W1WuXv7+/Qr2LFipKk9PR0l4+VmZmphISEYtVb3iUmJrq7BBQB56v43P0cJiYmKiAgwGl7XqzXUiSTSQMHDsyzT8WKAVq3bq0iIiKKXE9h+jir2Wjc/bOBwrsxEzjjMcGmbt26kqRp06bpwIEDWrVqlQICApSRkeHQLyfQBAUFuXwsPz8/+/G8jcViUWJioiIjIxUYGOjuclAAzlfJsVqtbj1+ZGSk6tevn6s9v7qy0q9JNpvTtWak/603U7lyZaf3nZ/CPB951WwUvL7Kl6NHjxaqn1uDTXJysnbs2KFu3brZx9H4+Piobt26unDhgiIiInThwgWHfXJuh4eHu3xck8lUrGBkBIGBgV7/HJQnnK/ic/cvrrzOYWHqKmitGVd+PgpzXG/5ufOWx1neFeZjKMnNg4fNZrNGjhypHTt22NsyMzP1yy+/KCoqSq1bt9aePXt0/fp1+/adO3fq9ttvV2hoqDtKBgAAHsytwaZevXq6++679cYbb2j37t06fPiwxo4dq99++039+/dXnz59dPXqVY0bN05Hjx7V+vXrtWLFCsXFxbmzbAAA4KHcvkDfnDlz1K5dO40YMUKPPvqoUlNT9cEHH+jWW29VaGioli1bphMnTqhXr15auHChRo8erV69erm7bAAA4IHcPng4ODhYkyZN0qRJk5xub9KkidasWVO2RQGAASQlJeW55pe3zwyFcbk92AAASl5SUpKi76wvq8W469AAzhBsAMCAzGazrJa0PKeKnz+xR4d/WO2GyoDS5VKw+fzzz3XfffcVaqEcAID75DVV/GryaTdUA5Q+lwYPjx49Wh06dNCkSZP0008/lXRNAAAALnEp2Gzbtk3PPvusdu7cqb59+6pHjx5avny5Ll68WNL1AQAAFJpLwSYiIkLPPfectmzZog8++ECtWrXS0qVL1aVLFw0ZMkRffPGFsrKySrpWAACAfBV78HCLFi3UokULPfroo/rrX/+q7du3a/v27QoLC9MzzzyjZ599VhUqVCiJWgEAAPJVrGBz5swZbdy4URs3blRSUpJq166tkSNHqnPnztq+fbveeecdHT16VDNnziypegGgXMpr3RjWkwFKlkvB5pNPPtHGjRu1d+9eVaxYUd27d9e0adPUqlUre5969eopJSVFH330EcEGgNeyXkuRTCbFxsa6uxTAK7gUbCZMmKCmTZtq0qRJ6tGjhypVquS0X3R0tPr27VusAgGgPMtKvybZbKwnA5QRl9exqVu3rq5fv24fP2O1WpWZmang4GB7v549e5ZIkQBQ3rGeDFA2XJoVFRkZqYkTJ+qxxx6zt+3du1ft2rXTzJkzlZ2dXWIFAgAAFJZLwWb+/Pn69NNP9eCDD9rbGjRooJdfflkff/yxli1bVmIFAgAAFJZLH0V99tlnGjNmjPr162dvq1y5svr37y9fX1+tXLlSgwcPLrEiAQAACsOlKzYpKSmqVauW02116tTRuXPnilUUAACAK1wKNnXq1NHWrVudbtu2bZtuu+22YhUFAADgCpc+inr66ac1duxYpaam6p577lFoaKiSk5P1zTffaPPmzZo+fXpJ1wkAAFAgl4JNz549de3aNS1atEhffPGFvb1KlSqaMGEC07wBAIBbuPyVCk8++aSeeOIJnThxQqmpqbr55ptVp04d+fi49OkWAABAsRXru6JMJpPq1KlTUrUAAAAUi0vBJjk5WdOmTdP27dtlsVhks9kctptMJv3yyy8lUiAAAEBhuRRspkyZom+++UYPPPCAIiIi+PgJAAB4BJeCzb///W+99tprfMElAADwKC5davHz88tzgT4AAAB3cSnY3Hvvvfr8889LuhYAAIBicemjqAYNGmjevHk6deqUmjZtqoCAAIftJpNJQ4cOLZECAQAACsvlwcOStHv3bu3evTvXdoINAABwB5eCzcGDB0u6DgAAgGIr9jztK1eu6NixY8rIyND169dLoiYAAACXuBxsdu3apUcffVQxMTF66KGHdOTIEY0aNUozZswoyfoAAAAKzaVgs2PHDg0cOFABAQF6+eWX7SsP33nnnVq5cqXef//9Ei0SAACgMFwKNvPmzdOf/vQn/eMf/9AzzzxjDzZDhgzRoEGD9Mknn5RokQAAAIXhUrBJSEhQnz59JP0+A+qPOnTooDNnzhS/MgAAgCJyKdgEBwfr4sWLTredPXtWwcHBxSoKAADAFS4Fmz/96U+aO3eu/vOf/9jbTCaTzp07p8WLF6tz584lVR8AAEChubSOzahRo3TgwAE99thjCgsLkySNHDlS586dU/Xq1TVy5MgSLRIAAKAwXAo2t9xyiz755BNt2LBBO3fuVGpqqoKDg/XUU0+pd+/eCgwMLOk6AQAACuRSsJEkf39/PfbYY3rsscdKsh4AAACXuRRsNmzYUGCfnj17unLXALxAUlKSzGZzntvDwsJUu3btMqwIRZXfOSzo/BVnX6AgLgWbsWPHOm03mUyqUKGCKlSoQLAB4FRSUpKi76wvqyUtzz4BgUE6dDCBX3AeqqBzmN/5K86+QGG4FGy+/vrrXG1paWmKj4/X0qVL9c477xS7MADGZDabZbWkqdn9IxQcUjPX9ivJp7V/81yZzWZ+uXmo/M5hQeevOPsCheFSsKlRo4bT9jvuuEOZmZmaOnWqVq9eXazCABhbcEhN3RIe5e4yUAzFOYecf5SWYn+7942io6P13//+t6TvFgAAoEAlGmwyMjK0du1ahYaGluTdAgAAFIpLH0V17do113dEZWdnKyUlRenp6RozZkyJFAcAAFAULgWbmJiYXMFGkipVqqQuXbqoffv2xS4MQPmW15TehISEQu2fVz+mA5cPeZ2/wp5/wFUuBZsZM2aUdB0ADKQwU7rzYr2WIplMio2Ndbqd6cCeraDzB5Q2l4LNr7/+WqT+t956qyuHAVBO5Tel9/yJPTr8Q96zJrPSr0k2G9OBy6n8zp9U8PkHiqvExtjkh0uPgHdyNqX3avJpl/dF+ZHX+Svs+Qdc5VKwmTdvniZOnKiGDRvq4YcfVnh4uFJSUrRt2zZt3rxZzz33XJ5r3QAAAJQWl4LNxo0b1aVLl1xjbXr06KHQ0FDt3btXL7zwQokUCAAAUFgurWOzY8cOPfjgg0633X333dqzZ0+xigIAAHCFS8GmSpUqOnDggNNtO3bsUHh4eLGKAgAAcIVLH0U98sgjevfdd2WxWNS1a1eFhITIbDZry5Yt+vDDDzVhwoSSrhMAUMZYiwblkUvB5vnnn9eVK1e0YsUKLV++XJJks9kUGBioESNGqF+/fiVaJACg7LAWDcozl4KNyWTS2LFj9fzzz2v//v26fPmyqlSpombNmqlSpUolXSMAoAyxFg3KM5eCTY5KlSqpWrVqkqRmzZopKyurRIoCALgfa9GgPHI52GzcuFFvvfWWLl68KJPJpE8++UQLFiyQn5+f3nrrLfn7+5dknQAAAAVyaVbUpk2bNGbMGLVt21Zz5sxRdna2JOnee+/Vt99+q0WLFpVokQAAAIXh0hWbxYsXq1+/fpo0aZKuX79ub+/Tp4+Sk5P18ccf66WXXiqpGgEAAArFpSs2J06c0L333ut0W9OmTXX+/PliFQUAAOAKl4JNaGiojh075nTbsWPHFBoaWqyiAAAAXOFSsOnRo4fmz5+vLVu2KCMjQ9LvU8B//vlnLVq0SN27dy/RIgEAAArDpTE2L730kg4fPqyXXnpJPj6/Z6OnnnpKaWlpatWqlV588cUSLRIAAKAwXAo2/v7+WrZsmb7//nvt3LlTqampCg4OVkxMjDp16iSTyVTo+0pNTdWcOXO0fft2Xb16VdHR0Ro1apRatWol6ffvnpo1a5aOHTum6tWra9iwYXrggQdcKRsAABicS8Fm4MCBGjRokDp06KAOHToUq4CRI0fq4sWLmjNnjkJDQ/WPf/xDAwcO1D//+U/ZbDbFxcVpwIABmjVrlrZv367Ro0crJCRE7dq1K9ZxAQCA8bgUbPbu3VukqzJ5OXnypL7//nutXr1aLVu2lCRNmDBB3333nT777DNdunRJ0dHRGjFihCQpKipKv/zyi5YtW0awAQAAubg0eLhjx4769NNPlZmZWayDV6lSRUuWLFHjxo3tbSaTSSaTSb/99pvi4+NzBZi2bdtqz549stlsxTo2AAAwHpeu2FSsWFGffvqpNm/erKioKAUFBTlsN5lM+vvf/17g/dx8883q1KmTQ9vWrVt18uRJvfbaa/rnP/+piIgIh+3VqlWTxWJRSkqKQkJCXClfNptNaWlpLu1b3lksFoe/4dlK4nydOnVKZrPZ6bawsDDVqlXL5fvOS2n/fO3fvz/PY2RkZDj9SpdDhw6Vak3uZLFYcr2nlefXuLPHU1rH+ePf8Gw2m61Qnxa5FGzOnTun5s2bOxzsxoO7Yu/evXr11Vd13333qXPnzrJarbneoHJu50wzd0VmZqYSEhJc3t8IEhMT3V0CisDV83Xu3Dn16fOI0tOtTrdXrBigdevW5voPRHGV1s+X9VqKZDJp4MCBefYxmXxks2WXyvE9VWJiogICAnK1lVfOHk9pHw/lQ2G+h7LQweaLL75Q27ZtdfPNN+sf//hHsQpz5quvvtLLL7+sFi1aaPbs2ZJ+vzJ0Y4DJuR0YGOjysfz8/FS3bl3Xiy3HLBaLEhMTFRkZWaznEGWjuOfLarUqPd2qZvePUHBITYdtV5JPa//muapcubLq169fUiXbj1sastKvSTab08cjSedP7NHhH1Y73Z6zzYgiIyNzncPSOgdlwdnjKQ28H5YvR48eLVS/QgebF198UWvWrFGTJk3sbUuXLlXv3r2LvdLwqlWrNG3aNHXv3l0zZ860J7Lq1avrwoULDn0vXLigoKAgBQcHu3w8k8mU6+MzbxMYGOj1z0F54ur5ynmzDg6pqVvCo0r0vgtz3NKS1+O5mnw6z+0524zI2Tksz7+oy/r9iffD8qGwk5YKPXj4xo+Xrl+/rjlz5ujcuXNFq+wGq1ev1tSpU/Xkk09qzpw5DpeZWrVqpR9//NGh/86dO9WiRQv7woAAAAA5XBpjk6O4M5NOnDihN998U/fee6/i4uIcBjgGBAToqaeeUq9evTR79mz16tVL3377rbZs2aJly5YV67gAAMCYihVsimvr1q3KzMzUl19+qS+//NJhW69evTRjxgwtWrRIs2bN0t///nfVrFlTs2bNYg0bAADglFuDzZAhQzRkyJB8+9x99926++67y6giAABQnhV7oEpJrEAMAABQEop0xWbo0KG55pAPGTJEfn5+Dm0mk0lfffVV8asDAAAogkIHm169epVmHQAAAMVW6GAzffr00qwDAACg2FgMBgAAGAbBBgAAGAbBBgAAGAbBBgAAGIZbF+gDABRfQkJCodoAb0CwAYByynotRTKZFBsb6+5SAI9BsAGAcior/Zpks6nZ/SMUHFLTYdv5E3t0+IfVbqoMcB+CDQCUc8EhNXVLeJRD29Xk026qBnAvBg8DAADDINgAAADDINgAAADDYIwN4OXymxYcFham2rVrl2E1AFA8BBvASxVmqnBAYJAOHUwg3AAoNwg2gJfKb6qwJF1JPq39m+fKbDYTbACUGwQbwMs5myoMAOUVg4cBAIBhEGwAAIBhEGwAAIBhEGwAAIBhMHgYAOAVkpKSZDab7bctFosSExNltVoVGBjIuk0GQbABABheUlKSou+sL6slLc8+rNtkDAQbAIDhmc1mWS1prNvkBQg2AACvwbpNxsfgYQAAYBgEGwAAYBgEGwAAYBiMsQGQr4SEBKftTI1FacnrZ04q/Z87ft7LP4INAKes11Ikk0mxsbFOtzM1FiWtoJ85qfR+7vh5Nw6CDQCnstKvSTab0+mxTI1FacjvZ04q3Z87ft6Ng2ADIF9Mj0VZc+fPHD/v5R+DhwEAgGEQbAAAgGEQbAAAgGEQbAAAgGEweBgAYBhJSUkym8252vNbGwfGQrABABhCUlKSou+sL6slzd2lwI0INgAAQzCbzbJa0pyuRXP+xB4d/mG1mypDWSLYAAAMxdlaNFeTT7upGpQ1Bg8DAADDINgAAADDINgAAADDINgAAADDYPAw4AHyWnvDYrEoMTFRlSpVUnR0dJH2lVi7A4D3IdgAblaYtTcCAgN16OBB1a5du8j7AoA3IdgAbpbf2huSdCX5tPZvniuz2Zwr2BS0L2t3APA2BBvAQzhbe6O4+7J2BwBvw+BhAABgGAQbAABgGAQbAABgGIyxAeCyvKaTM80cgLsQbAAUmfVaimQyKTY21t2lAIADgg2AIstKvybZbEwzB+BxCDYAXMY0cwCehsHDAADAMAg2AADAMAg2AADAMAg2AADAMBg8DAAoV1g/Cfkh2AAAygXWT0JhEGwAAOUC6yehMAg2AIByhfWTkB+PGjz8t7/9TU899ZRDW0JCgmJjY9WsWTN17dpVK1eudFN1AADA03lMsPnggw80b948h7aUlBQNGDBAtWvX1rp16zR06FDNnj1b69atc0+RAADAo7n9o6jz589r4sSJ2rVrlyIjIx22ffzxx/Lz89OUKVPk6+urqKgonTx5UkuWLFGfPn3cUzAAAPBYbr9i89///ld+fn769NNP1bRpU4dt8fHxiomJka/v//JX27ZtlZiYKLPZXNalAgAAD+f2KzZdu3ZV165dnW47d+6c6tWr59BWrVo1SdLZs2cVFhbm0jFtNpvS0tJc2re8s1gsDn/D/Qp7LiwWS66fW84jUHb279+f52suLCxMtWrVKuOKvIvNZpPJZCqwn9uDTX6sVqv8/f0d2ipWrChJSk9Pd/l+MzMzvX4hp8TERHeXgP9T2HORmJiogIAAl/YF4Lqc9XMGDhyYZ5+KFQO0bt1aRURElGFl3ufGTOCMRwebgIAAZWRkOLTlBJqgoCCX79fPz09169YtVm3llcViUWJioiIjIxUYGOjucqDfA3xhREZGqn79+i7tC8B1Ba2fcyX5tPZvnqvKlSvneo2i5Bw9erRQ/Tw62EREROjChQsObTm3w8PDXb5fk8lUrGBkBIGBgV7/HHiKwgZMZ+eMcAqUnbzWz8nB+2rpKszHUJIHDB7OT+vWrbVnzx5dv37d3rZz507dfvvtCg0NdWNlAADAE3l0sOnTp4+uXr2qcePG6ejRo1q/fr1WrFihuLg4d5cGAAA8kEcHm9DQUC1btkwnTpxQr169tHDhQo0ePVq9evVyd2kAAMADedQYmxkzZuRqa9KkidasWeOGagAAQHnj0VdsAAAAioJgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADINgAwAADMPX3QUA5UlSUpLMZrPTbWFhYapdu3apHTshIaFQbQDgzQg2QCElJSUp+s76slrSnG4PCAzSoYMJJR5urNdSJJNJsbGxJXq/AGBEBBugkMxms6yWNDW7f4SCQ2o6bLuSfFr7N8+V2Wwu8WCTlX5NstmcHvf8iT06/MPqEj0eAJRnBBugiIJDauqW8CiPOO7V5NNlXgcAeDIGDwMAAMMg2AAAAMMg2AAAAMNgjA28jjunbAPAjXhPKlkEG3gVd03ZBgBneE8qeQQbeBV3TdkGAGd4Typ5BBt4JXdN2QYAZ3hPKjkMHgYAAIZBsAEAAIZBsAEAAIZBsAEAAIbB4GGgBCUkJOS5jfUoAGPL6/XPa79sEWyAEmC9liKZTIqNjc2zD+tRAMZU0Ouf137ZItgAJSAr/Zpkszldi0JiPQrAyPJ7/fPaL3sEG6AEsRYF4L14/XsGBg8DAADDINgAAADDINgAAADDINgAAADDYPAwcIO81qLIb40aAMiPO95XkpKSZDab89xu1PV1CDbA/ynMWjQAUBTuel9JSkpS9J31ZbWk5dnHqOvrEGyA/1PQWjTnT+zR4R9Wu6EyAOWVu95XzGazrJY0r1xbi2AD3CCvtSiuJp92QzUAjMBd7yveuLYOg4cBAIBhEGwAAIBhEGwAAIBhMMYGhpPfFEembAMwkrze7wr7XpdXv/I8FZxgA0MpzBRHADCC4rzfFTQNvTxPBSfYwFAKmuLIlG0ARpHf+11B73X5TUMv71PBCTYwJKZsA/AWzt7vCvteZ8Tp4AweBgAAhkGwAQAAhkGwAQAAhkGwAQAAhsHg4RLkrV8Rn5f8no+Cnovi7OvJnK0Zwdo6APKT13sE7x3OEWxKiDd/RbwzBT0f+T0XxdnXUxW0ZgQA3Ij3DdcQbEqIN39FvDP5PR8FPRfF2ddT5bdmBGvrAHAmv/cNifeOvBBsSpgR1wQojuI8H0Z8Louz3gQA78S6XEXD4GEAAGAYBBsAAGAYBBsAAGAYBBsAAGAYDB5GvvJbTyY9PV0VK1Z0uq0w6ysUZ20G1nUAAPfx5LXGCDbIU0HryZhMPrLZsot8v8VZm4F1HQDAvTx9rTGCDfKU33oyOesnuLK+QnHWZmBdBwBwL09fa6xcBJvs7GwtXLhQn3zyia5cuaLWrVvr9ddfV61atdxdmlfIb+2V4qyv4K59AQDF56lrjZWLwcOLFi3S6tWrNXXqVH300UfKzs7WoEGDlJGR4e7SAACAB/H4YJORkaH33ntPw4cPV+fOnXXnnXdq7ty5OnfunL744gt3lwcAADyIxwebgwcP6tq1a2rXrp297eabb1aDBg20e/duN1YGAAA8jclms9ncXUR+vvjiCw0bNkwHDhxQQECAvf3FF1+U1WrV3/72tyLd3969e2Wz2eTn51eidaanp+vXX39VxaBb5FMh99Cl7OtZSk+7rKpVqzo9tslkUn6nIr/tRd03KytLvr6+Be6bmZmpixcvOn1M1zPTlWG9mufjzW+7u/b11LrYl33Zt/j7empd5XHfgn5f5fe7IWffW2+9Nc/lQFyVmZkpk8mkFi1a5NvP4wcPWywWSZK/v79De8WKFXX58uUi35/JZHL4u6QEBASoTp06BfQKLdFjlraAgAAFBwfn06NaAfeQ33Z37Vua982+7Mu+7t23NO/b2/bN+/dVwb8bSud3nclkKtTvbo8PNjlXaTIyMhyu2KSnpyswMLDI99e8efMSqw0AAHgWjx9jU716dUnShQsXHNovXLig8PBwd5QEAAA8lMcHmzvvvFOVKlXSrl277G2//fabfvnlF7Vu3dqNlQEAAE/j8R9F+fv7KzY2VrNnz1ZISIhq1KihWbNmKSIiQvfdd5+7ywMAAB7E44ONJA0fPlxZWVkaP368rFarWrdureXLl5f4zCYAAFC+efx0bwAAgMLy+DE2AAAAhUWwAQAAhkGwAQAAhkGwAQAAhkGwAQAAhkGwAQAAhkGwAQAAhkGwMbATJ06oefPmWr9+fZ59Pv30U0VHR+f6c/r06TKs1HudP3/e6fOf1zlLSUnRqFGj1Lp1a8XExGjy5MmyWCxlXLX3Kur54vXlfhs2bFCPHj3UuHFjPfDAA9q8eXOefdPT0zV58mS1a9dOzZs316hRo5ScnFyG1aIklIuVh1F0mZmZevnll5WWlpZvv0OHDikmJkZz5sxxaA8JCSnN8vB/Dh48qIoVK+qrr76SyWSytwcHBzvtP3z4cFksFq1YsUK//fabxo0bp7S0NM2cObOsSvZqRT1fvL7ca+PGjRo3bpxee+01dezYUf/61780cuRIRUREqHnz5rn6T5o0SfHx8VqwYIH8/f01ceJEDR8+XKtWrXJD9XAVwcagFixYoEqVKhXY7/Dhw4qOjlbVqlXLoCrc6PDhw4qMjFS1atUK7Ltv3z79+OOP2rRpk6KioiRJU6ZM0aBBgzRy5Ei+7b4MFOV85fTn9eUeNptNb7/9tp5++mk9+eSTkqTnnntO8fHx+vHHH3MFm/Pnz2vDhg1avHixWrVqJUmaM2eOunfvrn379jkNQvBMfBRlQLt379aaNWs0Y8aMAvseOnTI/ksSZa8oz398fLyqVq3q0D8mJkYmk0l79uwprRLxB0V9vfD6cp8TJ07ozJkzeuihhxzaly9frri4uFz9c15Dbdu2tbfdfvvtCg8P1+7du0u3WJQogo3B/Pbbbxo9erTGjx+v6tWr59v38uXLOn/+vOLj4/XQQw/prrvu0vPPP68TJ06UUbU4fPiwkpOT9eSTT6p9+/Z6/PHH9e9//9tp3/Pnz+c6p/7+/qpcubLOnj1bFuV6vaKcL15f7pXzPKelpWngwIFq166dHn30UW3bts1p//Pnz6tKlSqqWLGiQ3u1atV07ty5Uq8XJYdgYzCTJk1S8+bNc/0vxZkjR45I+v2S7fTp0zVv3jylp6friSeekNlsLu1SvV5WVpaOHz+uy5cva9iwYVqyZImaNWumwYMHa8eOHbn6WywW+fv752qvWLGi0tPTy6Jkr1bU88Xry72uXr0qSRozZowefPBBvffee+rQoYOef/55Xl8GxxgbA9mwYYPi4+P12WefFap/q1attGPHDlWpUsU+EHLhwoXq3Lmz1q9fr8GDB5dmuV7P19dXu3btUoUKFRQQECBJatSokY4cOaLly5erXbt2Dv0DAgKUkZGR637S09MVFBRUJjV7s6KeL15f7uXn5ydJGjhwoHr16iVJql+/vn755Re9//77RXp9BQYGln7BKDFcsTGQdevW6dKlS+rcubOaN29uH+w2ceJEDRo0yOk+ISEhDrM7AgMDVbNmTZ0/f75MavZ2N910k/2XZI477rjD6fMfERGhCxcuOLRlZGQoNTW10INZUTxFOV8Sry93yhlMX69ePYf2unXrOp1uHxERodTU1Fzh5sKFCwzML2cINgYye/Zsbdq0SRs2bLD/kX6fIjxt2rRc/desWaM2bdo4TAm/evWqEhMTVbdu3bIq22sdOXJELVq00K5duxzaf/75Z6fPf+vWrXXu3DmdPHnS3vbjjz9Kklq2bFm6xaLI54vXl3s1bNhQN910kw4cOODQfvjwYdWuXTtX/5YtWyo7O9thIP6JEyd0/vx5tW7dutTrRckh2BhIeHi4brvtNoc/khQaGqrw8HBdv35dFy9elNVqlSTdfffdys7O1ujRo3XkyBH95z//0bBhwxQSEqLevXu786F4haioKNWpU0dTpkxRfHy8jh07punTp2v//v167rnncp2vpk2bqkWLFhoxYoR++ukn7dy5U6+//rp69uzJ/yjLQFHPF68v9woICNCgQYP0zjvv6PPPP1dSUpLeffddff/99xowYIAk6eLFi7p27Zqk398/H3jgAY0fP167du3STz/9pJEjRyomJkbNmjVz4yNBkdlgaPXq1bOtW7fOZrPZbKdOnXK4bbPZbD///LNtwIABtpYtW9patGhhGzZsmO3XX391V7le5+LFi7axY8faOnToYGvcuLGtb9++tt27d9tsNufny2w224YNG2Zr1qyZrU2bNraJEyfarFaru8r3OkU9X7y+3O+9996zde3a1dawYUPbww8/bPvyyy/t2+rVq2ebP3++/fa1a9ds48aNs7Vq1crWqlUr28iRI23JycnuKBvFYLLZbDZ3hysAAICSwEdRAADAMAg2AADAMAg2AADAMAg2AADAMAg2AADAMAg2AADAMAg2AADAMAg2AADAMAg2gJc4duyYpk6dqm7duqlp06Zq2bKl+vXrp9WrVysrK6tEjnH69GlFR0dr/fr1Lt/HwoULFR0drTVr1jjdfujQITVq1EgjRozQ+vXrFR0d7fRLDfOyYMECRUdH59tn165dio6OzvW9UM5s3bpVTzzxRK72LVu2aPDgwerYsaMaNWqku+66Sy+++KJ++ukne5/jx4+rSZMmevzxx+VsrdTs7Gz169dPbdq00fnz57Vjxw79+c9/VmZmZiEeKeCdCDaAF9i0aZN69+6tffv2acCAAVqyZInmzJmjBg0a6M0339SwYcOc/mItqmrVqmnNmjXq3Lmzy/cRFxen6OhozZo1K9e3YF+/fl2vvfaaqlSpookTJ6pz585as2aN277d/NKlS5o8ebLGjRtnb8vKytKLL76okSNHKiQkRBMmTND777+vV155RWazWf369dOmTZskSXXq1NGwYcO0d+9erV69Otf9r1q1Svv27dPrr7+u8PBwtWvXTjVq1NCiRYvK7DEC5Y57v9EBQGk7evSorUmTJrahQ4faMjMzc23fsmWLrV69erZ//etfbqjOuZ9//tnWoEED23PPPefQvnTpUlu9evVs3377rcv3PX/+fFu9evXy7bNz505bvXr1bDt37sy339SpU21xcXEObQsWLLDVq1fPtmXLllz9r1+/bhsyZIgtJibGZrFYbDabzZaVlWXr06ePrXnz5g7fI3Xq1Clbs2bNbC+99JLDffz000+2Ro0a2c6fP59vbYC34ooNYHDLli2Tj4+PJk+eLF9f31zbu3Xrpp49ezq0RUdHa+HCherdu7eaNGmihQsXSpJ2796tgQMHqnXr1mrUqJG6du2qBQsWKDs7W1Luj6LWr1+vBg0a6MCBA+rbt68aN26sLl26aPny5fnW3LBhQw0aNEhff/21tmzZIklKSkrSggUL1LdvX9199932+7/xo6j4+HjFxsaqadOmiomJ0ZgxY5ScnJzv8T766CN169ZNTZo0UWxsrH799dd8+0tScnKy1q5dqwcffNDeZrFYtHz5cnXv3l3dunXLtY+Pj49eeukltWnTRpcuXZIkVahQQdOnT1dGRoYmTZpk7ztx4kTddNNNmjhxosN9NG7cWLfeeqvef//9AmsEvBHBBjC4r7/+Wm3btlVoaGiefWbOnKkePXo4tC1evFgPPfSQ5s+fr27duungwYPq37+/KleurLlz5+rdd99Vq1attHDhQm3evDnP+87OztZLL72kHj16aMmSJWrRooX++te/6rvvvsu37qFDh+qOO+7QjBkzZLFYNHXqVFWtWlVjxozJc5/du3erf//+CggI0Lx58/Taa6/pxx9/1NNPPy2r1ep0n1WrVmnixInq1KmTFi1apKZNm2rChAn51iZJX3zxhbKystSlSxd72w8//KC0tDSHsHOj6OhozZ8/XzVq1LC33XHHHXrhhRe0fft2bdu2TZs2bdL/+3//T9OmTVPlypVz3Uf37t31+eefF1gj4I1y//cNgGFcvnxZly9fVmRkZK5tNw4YNplMqlChgv12q1atNGDAAPvtDRs2qH379po1a5Z8fH7/P1GHDh20bds27dq1Sw888IDTGmw2m55//nk9+uijkqSWLVvqyy+/1Pbt29WxY8c8a/f399ebb76pfv366S9/+Yv27NmjVatW6aabbspzn7feeku33367/va3v9kfS9OmTfXAAw9o3bp1evLJJ3PVtmjRIvXo0UOvvfaaJOmuu+7S1atX9dFHH+V5HEnauXOnoqKiHOo5deqUJOV6vrOzs+1XtXL4+PjYn0dJGjRokL744gtNnz5dVqtVffv2VadOnZweu3Hjxlq8eLGOHTumqKiofOsEvA1XbAADu/GXaY6TJ0+qYcOGDn/uvfdehz7169d3uN2zZ08tXbpUmZmZOnjwoLZu3ar58+fr+vXrBc7Sad68uf3f/v7+CgkJUVpaWoH1N2nSRM8++6x2796tAQMGqGXLlnn2tVgsOnDggDp16iSbzaasrCxlZWWpVq1aioqK0vfff59rn+PHj+vSpUsOV10k6f777y+wtlOnTqlmzZoObXk932+//Xau5/udd95x6OPr66vp06fr7Nmz8vf3z/fKVM5xizIbDPAWXLEBDKxKlSoKCgrSmTNnHNqrV6+utWvX2m+/8847Onz4sEOfoKAgh9tWq1VTp07Vxo0blZWVpZo1a6p58+by9fUtcEZVQECAw20fH59Cz8Lq2LGjli5dmufVixy//fabsrOztXTpUi1dujTX9ooVK+Zqu3z5sqTfn6c/qlq1aoF1Xb16VYGBgQ5tt956qyTpzJkzuuOOO+ztTzzxhO655x777UceecTpfUZHR6tatWpq3bp1vlemco575cqVAusEvA3BBjC4rl276ptvvtHVq1dVqVIlSb9fNWncuLG9j7NxHDeaNm2atm7dqnnz5ql9+/b24NOuXbtSqbuobrrpJplMJvXv39/px2I3hhDpf4EmZyBvjtTU1AKPV6VKlVzBokOHDqpYsaK2bNniMOU9PDxc4eHhhXgUhZNXIAPAR1GA4Q0ePFhZWVkaP368MjIycm23Wq32sSH52bNnj9q0aaN77rnHHmp+/vlnJScn5/kRTFmqVKmSGjRooOPHj6tx48b2P3fccYcWLFjgdLG9yMhIVa9e3T7zKsc333xT4PFuvfVWnT171qEtODhYAwYM0IYNG/Tll1863e/GK2OuyFnfJ+cKEYD/4YoNYHA5i929+uqr6t27tx555BFFR0crKytL+/bt09q1a2U2mzVo0KB876dJkybavHmzPvzwQ0VFRengwYN69913ZTKZZLFYyujR5G/kyJEaPHiwRo0apYcffljXr1/Xe++9pwMHDuj555/P1d9kMunll1/WqFGjNH78eHXv3l379+/Xhx9+WOCxOnTooM2bN+vKlSsKDg62tw8fPlznzp3TsGHD1L17d917772qVq2aLl68qG+++UabN2+2L7bnqj179qhmzZq6/fbbXb4PwKgINoAX6Natmxo1aqQPP/xQa9eu1ZkzZ2Sz2VSrVi316NFD/fr1czpz6o/Gjh2rzMxMzZs3TxkZGapZs6aee+45HT16VNu2bdP169fL5sHk46677tLy5cu1cOFCDR8+XH5+fmrYsKHef/99NWvWzOk+Dz74oHx8fLRo0SJt3LhR9erV05QpUzRy5Mh8j9WlSxf5+vrqu+++c5gqX6FCBc2cOVMPPvigPvnkE82aNUtms1k33XST6tevr3Hjxqlnz55OPxorrO+++07du3d3eX/AyEy2wo7gAwA4mDp1qo4cOaKVK1eW2THj4+P17LPP6quvvnLbV0kAnowxNgDgoiFDhujgwYMOX2xZ2pYtW6ZnnnmGUAPkgWADAC6qWrWqJk2apDfffLNMjrdjxw79+uuvGjZsWJkcDyiP+CgKAAAYBldsAACAYRBsAACAYRBsAACAYRBsAACAYRBsAACAYRBsAACAYRBsAACAYRBsAACAYfx/M7Macli39D8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.517416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.333979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.305651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.301989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.517454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.748336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.394285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               GY\n",
       "count  766.000000\n",
       "mean     5.517416\n",
       "std      0.333979\n",
       "min      4.305651\n",
       "25%      5.301989\n",
       "50%      5.517454\n",
       "75%      5.748336\n",
       "max      6.394285"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "eyt1 = pyreadr.read_r('./data/eyt1.RData')\n",
    "\n",
    "# extract training example labels\n",
    "y = eyt1['Pheno_Disc_Env1'][['GY']]\n",
    "\n",
    "y = y.set_index(eyt1['Pheno_Disc_Env1']['GID'])\n",
    "\n",
    "y = y.sort_index()\n",
    "\n",
    "display(y.head())\n",
    "\n",
    "# check missing values\n",
    "cdt.assert_no_bad_values(y)\n",
    "\n",
    "# each seed was planted in 4 different environments, but we don't care about environmental differences\n",
    "# so we take the average of every group of four rows to reduce the dataset to 1/4 its original size\n",
    "y = cdt.avg_rows(y, 4)\n",
    "\n",
    "plot_GY_hist(y, 'GY, Unscaled')\n",
    "y.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c6a9b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID6569128</th>\n",
       "      <th>GID6688880</th>\n",
       "      <th>GID6688916</th>\n",
       "      <th>GID6688933</th>\n",
       "      <th>GID6688934</th>\n",
       "      <th>GID6688949</th>\n",
       "      <th>GID6689407</th>\n",
       "      <th>GID6689482</th>\n",
       "      <th>GID6689550</th>\n",
       "      <th>GID6738288</th>\n",
       "      <th>...</th>\n",
       "      <th>GID6939899</th>\n",
       "      <th>GID6939900</th>\n",
       "      <th>GID6939902</th>\n",
       "      <th>GID6939903</th>\n",
       "      <th>GID6939904</th>\n",
       "      <th>GID6939917</th>\n",
       "      <th>GID6939919</th>\n",
       "      <th>GID6939938</th>\n",
       "      <th>GID6939941</th>\n",
       "      <th>GID6939945</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GID6569128</th>\n",
       "      <td>0.788801</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.157880</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>-0.110899</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.127674</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.199459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688880</th>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>0.078890</td>\n",
       "      <td>-0.201346</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072171</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.061086</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.004447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688916</th>\n",
       "      <td>0.025987</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>1.170073</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.101532</td>\n",
       "      <td>-0.196780</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>-0.013459</td>\n",
       "      <td>0.126464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428609</td>\n",
       "      <td>0.423184</td>\n",
       "      <td>0.427788</td>\n",
       "      <td>0.408326</td>\n",
       "      <td>0.426844</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>0.163524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688933</th>\n",
       "      <td>-0.138795</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.021636</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>-0.080560</td>\n",
       "      <td>0.402479</td>\n",
       "      <td>-0.218803</td>\n",
       "      <td>-0.102718</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079312</td>\n",
       "      <td>-0.087824</td>\n",
       "      <td>-0.089912</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6688934</th>\n",
       "      <td>-0.157880</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.031717</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.996666</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>0.395843</td>\n",
       "      <td>-0.310471</td>\n",
       "      <td>-0.138902</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.026372</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>-0.154557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939917</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>-0.140529</td>\n",
       "      <td>-0.098509</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>-0.114305</td>\n",
       "      <td>0.062388</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144931</td>\n",
       "      <td>0.144932</td>\n",
       "      <td>0.155032</td>\n",
       "      <td>0.150037</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>1.112390</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.087428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939919</th>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>-0.141205</td>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.070163</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255040</td>\n",
       "      <td>0.249602</td>\n",
       "      <td>0.262775</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.986131</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.167608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939938</th>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.108757</td>\n",
       "      <td>0.240468</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.012778</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.149919</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719743</td>\n",
       "      <td>0.728707</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.708062</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.159823</td>\n",
       "      <td>1.118190</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.389739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939941</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.159136</td>\n",
       "      <td>-0.100318</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>-0.206353</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.084022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465269</td>\n",
       "      <td>0.461691</td>\n",
       "      <td>0.473004</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.121171</td>\n",
       "      <td>0.380622</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>1.070441</td>\n",
       "      <td>0.219306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID6939945</th>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>-0.154557</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.639670</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>1.145606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>766 rows Ã— 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  \\\n",
       "GID6569128    0.788801   -0.006443    0.025987   -0.138795   -0.157880   \n",
       "GID6688880   -0.006443    0.980542    0.064585   -0.168773   -0.081006   \n",
       "GID6688916    0.025987    0.064585    1.170073   -0.021636   -0.031717   \n",
       "GID6688933   -0.138795   -0.168773   -0.021636    0.879004    0.443678   \n",
       "GID6688934   -0.157880   -0.081006   -0.031717    0.443678    0.996666   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "GID6939917    0.004096    0.104630    0.006038   -0.140529   -0.098509   \n",
       "GID6939919    0.091188    0.113878    0.209395   -0.088961   -0.052304   \n",
       "GID6939938    0.074009    0.108757    0.240468   -0.096740   -0.012778   \n",
       "GID6939941    0.032992    0.154718    0.255337   -0.159136   -0.100318   \n",
       "GID6939945    0.199459    0.004447    0.163524   -0.108800   -0.154557   \n",
       "\n",
       "            GID6688949  GID6689407  GID6689482  GID6689550  GID6738288  ...  \\\n",
       "GID6569128    0.096213   -0.110899    0.013069   -0.040445    0.007931  ...   \n",
       "GID6688880    0.078890   -0.201346    0.124671    0.253505    0.013636  ...   \n",
       "GID6688916    0.101532   -0.196780    0.041900   -0.013459    0.126464  ...   \n",
       "GID6688933   -0.080560    0.402479   -0.218803   -0.102718   -0.002303  ...   \n",
       "GID6688934   -0.140766    0.395843   -0.310471   -0.138902    0.088169  ...   \n",
       "...                ...         ...         ...         ...         ...  ...   \n",
       "GID6939917    0.048248   -0.114305    0.062388    0.060255    0.034630  ...   \n",
       "GID6939919    0.139241   -0.141205    0.184798    0.070163    0.015030  ...   \n",
       "GID6939938    0.058980   -0.149919    0.000392    0.063573    0.090066  ...   \n",
       "GID6939941    0.045545   -0.206353    0.194754    0.043889    0.084022  ...   \n",
       "GID6939945    0.152167   -0.163107    0.160584    0.030285    0.010552  ...   \n",
       "\n",
       "            GID6939899  GID6939900  GID6939902  GID6939903  GID6939904  \\\n",
       "GID6569128    0.125612    0.133808    0.137456    0.127674    0.130468   \n",
       "GID6688880    0.072171    0.061650    0.057898    0.079085    0.061086   \n",
       "GID6688916    0.428609    0.423184    0.427788    0.408326    0.426844   \n",
       "GID6688933   -0.079312   -0.087824   -0.089912   -0.067028   -0.084206   \n",
       "GID6688934   -0.016690   -0.017375   -0.026372   -0.014478   -0.016350   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "GID6939917    0.144931    0.144932    0.155032    0.150037    0.161459   \n",
       "GID6939919    0.255040    0.249602    0.262775    0.242206    0.253641   \n",
       "GID6939938    0.719743    0.728707    0.732558    0.708062    0.704652   \n",
       "GID6939941    0.465269    0.461691    0.473004    0.432339    0.435412   \n",
       "GID6939945    0.660434    0.664936    0.645452    0.639670    0.649749   \n",
       "\n",
       "            GID6939917  GID6939919  GID6939938  GID6939941  GID6939945  \n",
       "GID6569128    0.004096    0.091188    0.074009    0.032992    0.199459  \n",
       "GID6688880    0.104630    0.113878    0.108757    0.154718    0.004447  \n",
       "GID6688916    0.006038    0.209395    0.240468    0.255337    0.163524  \n",
       "GID6688933   -0.140529   -0.088961   -0.096740   -0.159136   -0.108800  \n",
       "GID6688934   -0.098509   -0.052304   -0.012778   -0.100318   -0.154557  \n",
       "...                ...         ...         ...         ...         ...  \n",
       "GID6939917    1.112390    0.077593    0.107666    0.121171    0.087428  \n",
       "GID6939919    0.077593    0.986131    0.159823    0.380622    0.167608  \n",
       "GID6939938    0.107666    0.159823    1.118190    0.388284    0.389739  \n",
       "GID6939941    0.121171    0.380622    0.388284    1.070441    0.219306  \n",
       "GID6939945    0.087428    0.167608    0.389739    0.219306    1.145606  \n",
       "\n",
       "[766 rows x 766 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# temporary data override for quick testing\\ny_bin = pd.DataFrame(data={'GY': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]})\\nX = pd.DataFrame(data={'rs1': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40],})\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract feature matrix and set index to match y\n",
    "X = eyt1['Geno_Env1'].sort_index()\n",
    "\n",
    "display(X)\n",
    "\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "# temporary data override for quick testing\n",
    "y_bin = pd.DataFrame(data={'GY': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]})\n",
    "X = pd.DataFrame(data={'rs1': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40],})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c222e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_and_target(X: pd.DataFrame, y: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, StandardScaler, StandardScaler):\n",
    "    \"\"\"\n",
    "    Scale the feature matrix and target values using StandardScaler.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.DataFrame): Target values.\n",
    "    \n",
    "    Returns:\n",
    "        X_sc (pd.DataFrame): Scaled feature matrix.\n",
    "        y_sc (pd.DataFrame): Scaled target values.\n",
    "        X_scaler (StandardScaler): Scaler used for features.\n",
    "        y_scaler (StandardScaler): Scaler used for target.\n",
    "    \"\"\"\n",
    "    X_sc, y_sc, X_scaler, y_scaler = None, None, None, None\n",
    "\n",
    "    # scale feature matrix\n",
    "\n",
    "    if X is not None:\n",
    "        X_scaler = StandardScaler()\n",
    "        X_sc = X_scaler.fit_transform(X)\n",
    "        X_sc = pd.DataFrame(X_sc, index=X.index, columns=X.columns)\n",
    "\n",
    "    if y is not None:\n",
    "        y_scaler = StandardScaler()\n",
    "        y_sc = y_scaler.fit_transform(y)\n",
    "        y_sc = pd.DataFrame(y_sc, index=y.index, columns=y.columns)\n",
    "    \n",
    "    return X_sc, y_sc, X_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11428d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smogn_prep(X: pd.DataFrame, y: pd.DataFrame, top_threshold_quantile: float, undersample: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset using the SMOGN algorithm\n",
    "    Created: 2024/02/05\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        GY_df (pd.DataFrame): DataFrame containing grain yield values\n",
    "        top_threshold_quantile (float): The quantile value to use as the threshold for the top class\n",
    "    \"\"\"\n",
    "\n",
    "    # temporarily combine X and y for compability with smogn library\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    smogn_X_y = pd.concat([X, y], axis=1)\n",
    "\n",
    "    # get GY distribution points\n",
    "    gy_min = y['GY'].min()\n",
    "    gy_max = y['GY'].max()\n",
    "    gy_just_under_threshold = y['GY'].quantile(top_threshold_quantile - 0.0001)\n",
    "    gy_just_over_threshold = y['GY'].quantile(top_threshold_quantile + 0.0001)\n",
    "\n",
    "    ctrl_points = [\n",
    "        [gy_min, 0, 0],\n",
    "        [gy_just_under_threshold, 0, 0],\n",
    "        [gy_just_over_threshold, 1, 0],\n",
    "        [gy_max, 1, 0]\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    display(smogn_X_y)\n",
    "\n",
    "    n_tries = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            X = smoter(\n",
    "            data=smogn_X_y,\n",
    "            y='GY',\n",
    "            k=5,\n",
    "            under_samp=undersample,\n",
    "            samp_method='balance',\n",
    "            rel_thres=top_threshold_quantile,\n",
    "            rel_method='manual',\n",
    "            rel_ctrl_pts_rg=ctrl_points,\n",
    "            rel_xtrm_type='high',\n",
    "            rel_coef=1.50\n",
    "        )\n",
    "            done = True\n",
    "\n",
    "        except ValueError:\n",
    "            if n_tries < 5:\n",
    "                n_tries += 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # split X and y back into separate DataFrames\n",
    "    y = X[['GY']]\n",
    "    X = X.drop(columns=['GY'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "589e47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_boundary_val = y[\"GY\"].quantile(TOP_THRESHOLD_QUANTILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f8edf",
   "metadata": {},
   "source": [
    "# Model R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_R(n_splits: int, X : pd.DataFrame, y : pd.DataFrame, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback, kfold_random_state: int, plot_title: str = \"\", **kwargs):\n",
    "    \"\"\"Perform inner cross-validation with grid search to find the best model parameters.\n",
    "    Created: 2024/12/03\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        Number of splits for KFold cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Parameter grid for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Parameter grid for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Callback function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for KFold shuffling.\n",
    "    plot_title : str, optional\n",
    "        Title for the plot (default is \"\").\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments for the model training.\n",
    "    Returns:\n",
    "    --------\n",
    "    avg_best_param1 : float\n",
    "        Average best parameter value for the first axis over all folds.\n",
    "    avg_best_param2 : float\n",
    "        Average best parameter value for the second axis over all folds.\"\"\"\n",
    "\n",
    "    # Create KFold object for inner-fold cross-validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Store best parameters (C, gamma) for each fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # print(f\"    INNER FOLD {i}\")\n",
    "        \n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train numpy 2D array of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "\n",
    "        # Use trained models to predict test set labels, and store in 2D array with each cell corresponding to a model with specific combination of parameters\n",
    "        y_preds_grid = bmo.grid_predict(X_test, model_grid)\n",
    "\n",
    "        # Create 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_test_grid = cdt.np_array_of_dfs(y_test, y_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating 2D array of pearson coefficients\n",
    "        pearson_grid = bmo.calculate_pearson_coefficients(y_preds_grid, y_test_grid)\n",
    "\n",
    "        # Find index of best pearson coefficient in the 2d array of pearson coefficients\n",
    "        best_row, best_col = np.unravel_index(np.argmax(pearson_grid), pearson_grid.shape)\n",
    "        \n",
    "        # Store hyperparameters of most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "        # print(f\"    Best parameters found: \\n{best_params.loc[i]}\")\n",
    "\n",
    "        # Create grid of scatter plots with predictions vs actuals, coloured by pearson coefficient for each model\n",
    "        scatter_grid = plot_shaded_scatter_grids(y_preds_grid, y_test_grid, axis1_params, axis2_params, pearson_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\Model R {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "        plt.show(scatter_grid)\n",
    "        plt.close(scatter_grid)\n",
    "\n",
    "    # calculate average best parameters over all inner folds to return to outer CV\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c111e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_R(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "               axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback : callable, kfold_random_state: int, top_boundary_val : float,\n",
    "               smogn_preprocess = False, undersample = True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation with an outer and inner loop to evaluate model performance.\n",
    "    Created: 2024/12/03\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation loop.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation loop.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object containing parameter list for the first hyperparameter axis (horizontal).\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object containing parameter list for the first hyperparameter axis (vertical).\n",
    "    train_model_callback : callable\n",
    "        Function to train the model. Should accept X, y, and hyperparameters as arguments.\n",
    "    kfold_random_state : int\n",
    "        Random state for reproducibility in KFold splitting.\n",
    "    top_line_thresh : float\n",
    "        Threshold to classify predictions as top or not top.\n",
    "    **kwargs\n",
    "        Additional arguments to pass to the train_model_callback function.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing 5 metrics (Pearson, F1 Score, Sensitivity, Specificity, Kappa) for each outer fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create KFold object for outer loop to split data into train and test sets\n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    kfold_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "    # Create arrays to store outer-fold final \"super model\"'s predictions and actuals\n",
    "    super_model_preds = [None] * n_outer_splits\n",
    "    super_model_actuals = [None] * n_outer_splits \n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # print(f\"OUTER CV: {train_model_callback.__name__}, FOLD {i}\")\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if smogn_preprocess:\n",
    "            top_boundary_quantile_in_train_set = percentileofscore(y_train.to_numpy().flatten(), top_boundary_val, kind='mean') / 100\n",
    "            # print(f'top_boundary_quantile_in_train_set: {top_boundary_quantile_in_train_set}')\n",
    "            # print(f\"X_train shape: {X_train.shape}\")\n",
    "            # print(f\"y_train shape: {y_train.shape}\")\n",
    "            # print(f\"X_train head: \\n{X_train.head()}\")\n",
    "            # print(f\"y_train head: \\n{y_train.head()}\")\n",
    "            # print(f\"y_train summary statistics:\")\n",
    "            # display(y_train.describe())\n",
    "            X_train_pre_smogn = X_train.copy()\n",
    "            y_train_pre_smogn = y_train.copy()\n",
    "            X_train, y_train = smogn_prep(X_train, y_train, top_boundary_quantile_in_train_set, undersample)\n",
    "\n",
    "            if not undersample:\n",
    "                # Unfortunately the SMOGN library for some reason only returns augmented data when undersampling is disabled. \n",
    "                # This means we need to manually concatenate the original data below the augmentation threshold with the \n",
    "                # augmented data. Read more: https://github.com/nickkunz/smogn/issues/21\n",
    "                non_augmented_indices = y_train_pre_smogn[y_train_pre_smogn < top_boundary_val].index\n",
    "                X_train_non_augmented = X_train_pre_smogn.loc[non_augmented_indices]\n",
    "                y_train_non_augmented = y_train_pre_smogn.loc[non_augmented_indices]\n",
    "                X_train = pd.concat([X_train, X_train_non_augmented], axis=0)\n",
    "                y_train = pd.concat([y_train, y_train_non_augmented], axis=0)\n",
    "                \n",
    "                # print shapes, first few rows of dfs, and stuff for debugging\n",
    "                # print(f\"X_train shape: {X_train.shape}\")\n",
    "                # print(f\"y_train shape: {y_train.shape}\")\n",
    "                # print(f\"X_train head: \\n{X_train.head()}\")\n",
    "                # print(f\"y_train head: \\n{y_train.head()}\")\n",
    "                # print(f\"y_train summary statistics:\")\n",
    "                # display(y_train.describe())\n",
    "\n",
    "            plot_GY_hist(y_train, f'Model R SMOGN-Augmented GY Histogram, Outer Fold {i}')\n",
    "        else:\n",
    "            plot_GY_hist(y_train, f'Model R Histogram, Outer Fold {i}')\n",
    "\n",
    "        print(f'y_train summary statistics:')\n",
    "        display(y_train.describe())\n",
    "\n",
    "        X_train, y_train, X_scaler, y_scaler = scale_features_and_target(X_train, y_train)\n",
    "        top_boundary_val_scaled = y_scaler.transform([[top_boundary_val]])[0, 0]\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "        y_test = y_scaler.transform(y_test)\n",
    "            \n",
    "        # Find mean best hyperparameter values based on prediction accuracy using inner-fold CV\n",
    "        best_param1, best_param2 = inner_CV_R(n_inner_splits, X_train, y_train, axis1_params, axis2_params, train_model_callback, kfold_random_state, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "        # print(f\"Average best parameters: {best_param1}, {best_param2}\")\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train, np.ravel(y_train), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set\n",
    "        y_pred = pd.DataFrame(super_model.predict(X_test), index=y_test.index, columns=y_test.columns)\n",
    "        plot_GY_hist(y_pred, f'Model R Predicted GY Histogram, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "\n",
    "        # Calculate pearson coefficient of continuous predictions\n",
    "        pearson, _ = pearsonr(np.ravel(y_pred), np.ravel(y_test))\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean)\n",
    "        # print(f\"Predictions: \\n{y_pred}\")\n",
    "        # print(f\"Actuals: \\n{y_test}\")\n",
    "\n",
    "        # print(\"y_pred: \", y_pred)\n",
    "        y_pred_top = bmo.continuous_to_binary_absolute(y_pred, top_boundary_val_scaled)\n",
    "        # print(f'y_pred_top: {y_pred_top}')\n",
    "        super_model_preds[i] = y_pred_top\n",
    "        y_test_top = bmo.continuous_to_binary_absolute(y_test, top_boundary_val_scaled)\n",
    "        super_model_actuals[i] = y_test_top\n",
    "\n",
    "        # print(f\"Top Boundary Value (Scaled): \\n{top_boundary_val_scaled}\")\n",
    "        print(f\"Binary Predictions: \\n{y_pred_top}\")\n",
    "        print(f\"Binary Actuals: \\n{y_test_top}\")\n",
    "\n",
    "        # plot super model predictions vs actuals scatterplot\n",
    "        cmp.plot_classification_results(y_pred, y_test, y_pred_top, y_test_top, \n",
    "                                        [f\"Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}\"],\n",
    "                                        save_path=f'{storage_dir}\\\\Super Model Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}.svg')\n",
    "\n",
    "        # Calculate classification metrics and add new row to kfold_metrics\n",
    "        classification_metrics = cdt.classification_metrics(y_pred_top, y_test_top)\n",
    "        print(classification_metrics)\n",
    "        pearson_df = pd.DataFrame([pearson], columns=['Pearson'])\n",
    "        metrics_row = pd.concat([pearson_df, classification_metrics], axis=1)\n",
    "        kfold_metrics = pd.concat([kfold_metrics, metrics_row], axis=0)\n",
    "\n",
    "    # Label each row of kfold_metrics with the fold number \n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    \n",
    "    return kfold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40ce7c",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Real values \n",
    "x_params_SVM_R = mdo.AxisParams('gamma', bmo.power_list(2, -14, -6))\n",
    "y_params_SVM_R = mdo.AxisParams('C', bmo.power_list(2, -2, 6))\n",
    "metrics_SVM_R = outer_CV_R(n_outer_splits=5, \n",
    "                           n_inner_splits=10, \n",
    "                           X=X, \n",
    "                           y=y,\n",
    "                           axis1_params=x_params_SVM_R, \n",
    "                           axis2_params=y_params_SVM_R, \n",
    "                           train_model_callback=bmo.train_SVM_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           undersample=UNDERSAMPLE,\n",
    "                           kfold_random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           kernel='rbf')\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for quick debugging tests  \n",
    "x_params_SVM_R = mdo.AxisParams('gamma', bmo.power_list(2, -1, 0))  \n",
    "y_params_SVM_R = mdo.AxisParams('C', bmo.power_list(2, 1, 2))  \n",
    "metrics_SVM_R = outer_CV_R(n_outer_splits=2,   \n",
    "                           n_inner_splits=2,   \n",
    "                           X=X,  \n",
    "                           y=y,    \n",
    "                           axis1_params=x_params_SVM_R,   \n",
    "                           axis2_params=y_params_SVM_R,   \n",
    "                           train_model_callback=bmo.train_SVM_regressor,   \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,  \n",
    "                           kfold_random_state=RANDOM_STATE,   \n",
    "                           top_line_thresh=TOP_THRESHOLD_QUANTILE,   \n",
    "                           kernel='rbf')  \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec953e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for best model from each outer fold\n",
    "display(metrics_SVM_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save average of each metric\n",
    "metrics_SVM_R_mean = metrics_SVM_R.mean().to_frame().T\n",
    "R_average_metrics.loc['SVM'] = metrics_SVM_R_mean.iloc[0]\n",
    "display(metrics_SVM_R_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddd5e6",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3dd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test values\n",
    "x_params_XGB_R = mdo.AxisParams('n_estimators', [13, 25, 50, 100, 200])\n",
    "y_params_XGB_R = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16])\n",
    "metrics_XGB_R = outer_CV_R(n_outer_splits=5, \n",
    "                           n_inner_splits=10, \n",
    "                           X=X, \n",
    "                           y=y,  \n",
    "                           axis1_params=x_params_XGB_R, \n",
    "                           axis2_params=y_params_XGB_R, \n",
    "                           train_model_callback=bmo.train_XGB_regressor, \n",
    "                           smogn_preprocess=SMOGN_PREPROCESS,\n",
    "                           undersample=UNDERSAMPLE,\n",
    "                           kfold_random_state=RANDOM_STATE, \n",
    "                           top_boundary_val=top_boundary_val, \n",
    "                           objective=\"reg:squarederror\", eval_metric=\"rmse\")\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for quick debugging tests\n",
    "x_params_XGB_R = mdo.AxisParams('n_estimators', [1, 2])\n",
    "y_params_XGB_R = mdo.AxisParams('max_depth', [1, 2])\n",
    "metrics_XGB_R = outer_CV_R(2, 2,  X_sc, y_sc, X_smogn, y_smogn, x_params_XGB_R, y_params_XGB_R, bmo.train_XGB_regressor, kfold_random_state=RANDOM_STATE, random_state=RANDOM_STATE, objective=\"reg:squarederror\", eval_metric=\"rmse\", top_line_thresh=TOP_THRESHOLD_QUANTILE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for best model from each outer fold\n",
    "display(metrics_XGB_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f701d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric and store results for analysis\n",
    "metrics_XGB_R_mean = metrics_XGB_R.mean().to_frame().T\n",
    "display(metrics_XGB_R_mean)\n",
    "R_average_metrics.loc['XGB'] = metrics_XGB_R_mean.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_R.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b59b6d",
   "metadata": {},
   "source": [
    "# Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8001b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_B(n_splits: int, X : pd.DataFrame, y_bin : pd.DataFrame, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, \n",
    "               train_model_callback, kfold_random_state: int, classification_col : int, plot_title: str = \"\", \n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Perform inner cross-validation to tune model hyperparameters and find the optimal classification threshold.\n",
    "    Created: 2024/12/04\n",
    "    Parameters:\n",
    "    n_splits (int): Number of splits for KFold cross-validation.\n",
    "    X (pd.DataFrame): Feature data.\n",
    "    y (pd.DataFrame): Target data.\n",
    "    axis1_params (mdo.AxisParams): Object containing hyperparameter values for the first (horizontal) axis.\n",
    "    axis2_params (mdo.AxisParams): Object containing hyperparameter values for the second (vertical) axis.\n",
    "    train_model_callback (function): Callback function to train the model.\n",
    "    kfold_random_state (int): Random state for KFold shuffling.\n",
    "    classification_col (int): Column index to pull classification proabibilities from - 0 for not top, 1 for top.\n",
    "    top_thresh_quantile (float): Threshold to classify predictions as top or not top.\n",
    "    plot_title (str, optional): Title for the ROC plot. Defaults to \"\".\n",
    "    **kwargs: Additional arguments for the train_model_callback function.\n",
    "    Returns:\n",
    "    tuple: Average best parameters (param1, param2) and the best classification threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Arrays to store parameters and binary classification thresholds of most accurate model for each inner fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "    best_thresholds = pd.DataFrame(columns=['threshold'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        print(f\"    INNER FOLD {i}\")\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_bin_train, y_bin_test = y_bin.iloc[train_index], y_bin.iloc[test_index]\n",
    "\n",
    "        print('X train:')\n",
    "        print(X_train)\n",
    "\n",
    "        print('y_train_bin:')\n",
    "        print(y_bin_train)\n",
    "\n",
    "        \n",
    "        # Train numpy 2D array of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_bin_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "        \n",
    "        # Use trained models to predict test set probabilities, and store in 2D array with each cell corresponding to a model with specific combination of parameters\n",
    "        y_proba_preds_grid = bmo.grid_predict_proba(X_test, model_grid, classification_col)\n",
    "\n",
    "        # Use probabilities to classify predictions as top or not top. This isn't the final classification, \n",
    "        # but a step towards finding the optimal threshold, so we just use the default 0.5 threshold for now.\n",
    "        y_binary_preds_grid = bmo.continuous_to_binary_absolute_grid(y_proba_preds_grid, 0.5)\n",
    "\n",
    "        # Create 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_bin_test_grid = cdt.np_array_of_dfs(y_bin_test, y_proba_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating 2D array of f1 scores.\n",
    "        f1_grid = bmo.calculate_f1_scores(y_binary_preds_grid, y_bin_test_grid)\n",
    "\n",
    "        # Find index of best f1 score in the 2d array of f1 scores\n",
    "        best_row, best_col = np.unravel_index(np.argmax(f1_grid), f1_grid.shape)\n",
    "        print(\"     Best row, best col: \\n\", best_row, best_col)\n",
    "        print(\"     Inner CV Probability Predictions at best row, best col: \\n\", y_proba_preds_grid[best_row, best_col])\n",
    "        print(\"     Inner CV binary predictions at best row, best col: \\n\", y_binary_preds_grid[best_row, best_col])\n",
    "        print(\"     Inner CV actuals at best row, best col: \\n\", y_bin_test_grid[best_row, best_col])\n",
    "        print(\"     Inner CV f1 score at best row, best col: \\n\", f1_grid[best_row, best_col])\n",
    "\n",
    "        # Store hyperparameters of most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "        print(f\"    Best parameters found: \\n{best_params.loc[i]}\")\n",
    "\n",
    "        # Find classification probability threshold between zero and one that yields lowest squared difference between sensitivity and \n",
    "        # specificity using this optimal model. To do this, we feed find_optimal_threshold() the probabilities predicted by the model, not the binary predictions.\n",
    "        best_model_proba_preds = y_proba_preds_grid[best_row, best_col]\n",
    "        best_thresholds.iloc[i, 0] = bmo.find_optimal_threshold_absolute(y_bin_test, best_model_proba_preds)\n",
    "        print(f\"    Best probability threshold found: {best_thresholds.iloc[i, 0]}\")\n",
    "\n",
    "        # Create grid of ROC plots with predictions vs actuals, coloured by f1 score for each model\n",
    "        roc_grid = plot_shaded_roc_grids(y_proba_preds_grid, y_bin_test_grid, axis1_params, axis2_params, f1_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\model_B, {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "\n",
    "        plt.show(roc_grid)\n",
    "        plt.close(roc_grid)\n",
    "\n",
    "    # Calculate average best parameters over all inner folds to return to outer CV\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    # Calculate average best threshold over all folds\n",
    "    best_threshold = best_thresholds['threshold'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2, best_threshold\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fba289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_B(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "               axis1_params: mdo.AxisParams, \n",
    "               axis2_params: mdo.AxisParams, train_model_callback : callable, random_state: int, \n",
    "               classification_col : int, top_boundary_val : float, smote_preprocess = False, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation with an outer and inner loop to evaluate model B performance.\n",
    "    Created: 2024/12/29\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for reproducibility in KFold.\n",
    "    classification_col : int\n",
    "        Column index to pull classification probabilities from - 0 for not top, 1 for top.\n",
    "    top_line_thresh : float\n",
    "        Threshold to classify predictions as top or not top.\n",
    "    **kwargs : dict\n",
    "        Additional parameters for the model training function.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the metrics for each outer fold, including F1 Score, Sensitivity, Specificity, and Kappa.\n",
    "    \"\"\"\n",
    "    \n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "    # Store metrics of best model for each fold\n",
    "    kfold_metrics = pd.DataFrame(columns=['F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        print(f\"OUTER CV: {train_model_callback.__name__}, FOLD {i}\")\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        y_train_bin = bmo.continuous_to_binary_absolute(y_train, top_boundary_val)\n",
    "        y_test_bin = bmo.continuous_to_binary_absolute(y_test, top_boundary_val)\n",
    "\n",
    "        # Print shape of data and number of top and not top samples\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"y_train shape: {y_train.shape}\")\n",
    "        print(f\"Number of top samples in y_train: {y_train_bin.sum()}\")\n",
    "        \n",
    "        if smote_preprocess:\n",
    "            sm = SMOTE(random_state=random_state)\n",
    "            X_train, y_train_bin = sm.fit_resample(X_train, y_train_bin)\n",
    "            print(f\"X_train shape after SMOTE: {X_train.shape}\")\t\n",
    "            print(f\"y_train shape after SMOTE: {y_train_bin.shape}\")\n",
    "            print(f\"Number of top samples in y_train after SMOTE: {y_train_bin.sum()}\")\n",
    "            \n",
    "        X_train, _, X_scaler, _ = scale_features_and_target(X_train, None)\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "\n",
    "        # Find average best parameters and threshold based on f1 score using inner-fold CV\n",
    "        best_param1, best_param2, best_threshold = inner_CV_B(n_inner_splits, X_train, y_train_bin, axis1_params, axis2_params, train_model_callback, random_state, classification_col, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "\n",
    "        print(f\"Average best parameters: {best_param1}, {best_param2}\")\n",
    "        print(f\"Average best threshold: {best_threshold}\")\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train, np.ravel(y_train_bin), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set probabilities\n",
    "        y_pred_proba = pd.DataFrame(super_model.predict_proba(X_test)[:, classification_col], index=y_test_bin.index, columns=y_test_bin.columns)\n",
    "        plot_GY_hist(y_pred_proba, f'Top Line Probability Histogram, {train_model_callback.__name__}, Outer Fold {i}', x_ax_label='Top Line Probability')\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean) using the best threshold as determined by inner CV\n",
    "        y_pred_bin = bmo.continuous_to_binary_absolute(y_pred_proba, best_threshold)\n",
    "        print(f\"Probability Predictions: \\n{y_pred_proba}\")\n",
    "        print(f\"Binary Predictions: \\n{y_pred_bin}\")\n",
    "        print(f\"Actuals: \\n{y_test_bin}\")\n",
    "\n",
    "        # Plot ROC and PR curves using seaborn\n",
    "        cmp.sns_plot_roc_curve(pd.DataFrame(y_test_bin), pd.DataFrame(y_pred_proba), f'Model B ROC Curve, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "        plt.savefig(f'{storage_dir}\\\\Model B ROC Curve, {train_model_callback.__name__}, Outer Fold {i}.svg', format=\"svg\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        cmp.sns_plot_pr_curve(pd.DataFrame(y_test_bin), pd.DataFrame(y_pred_proba), f'Model B PR Curve, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "        plt.savefig(f'{storage_dir}\\\\Model B PR Curve, {train_model_callback.__name__}, Outer Fold {i}.svg', format=\"svg\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "        # Calculate classification metrics and add new row to kfold_metrics   \n",
    "        classification_metrics = cdt.classification_metrics(y_pred_bin, y_test_bin)   \n",
    "        kfold_metrics = pd.concat([kfold_metrics, classification_metrics], axis=0)   \n",
    "\n",
    "    # Label each row of kfold_metrics with the fold number\n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    return kfold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7457c6",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b684319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER CV: train_SVM_classifier, FOLD 0\n",
      "    INNER FOLD 0\n",
      "X train:\n",
      "     GID6569128  GID6688880  GID6688916  GID6688933  GID6688934  GID6688949  \\\n",
      "0      6.043384   -0.153116   -0.122532   -0.699104   -0.653973    0.916929   \n",
      "1     -0.469112    9.853762    0.118903   -0.907374   -0.172494    0.676751   \n",
      "4     -1.709276   -0.909105   -0.483473    3.347600    6.577269   -2.368683   \n",
      "5      0.371567    0.712057    0.350003   -0.294516   -0.546787   12.643571   \n",
      "6     -1.324536   -2.129209   -1.515949    3.061372    2.814146   -1.308138   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "757    0.679440    0.537269    2.361952   -0.344987    0.226043    0.895713   \n",
      "758    0.709315    0.499227    2.390749   -0.359492    0.169695    0.881762   \n",
      "763    0.189733    1.014877    1.219057   -0.406926    0.254841    0.400715   \n",
      "764   -0.146169    1.480871    1.312059   -0.840420   -0.293449    0.214441   \n",
      "765    1.217080   -0.042709    0.737770   -0.490716   -0.633165    1.692704   \n",
      "\n",
      "     GID6689407  GID6689482  GID6689550  GID6738288  ...  GID6939899  \\\n",
      "0     -0.352935   -0.093001   -0.401814    0.000401  ...    0.287520   \n",
      "1     -0.944671    0.747830    2.480772    0.068439  ...    0.040285   \n",
      "4      2.962346   -2.530633   -1.367319    0.957425  ...   -0.370823   \n",
      "5     -0.047892    0.688321    0.691747   -0.285481  ...    0.173730   \n",
      "6      6.246700   -1.723761   -0.665256    0.296832  ...   -1.182666   \n",
      "..          ...         ...         ...         ...  ...         ...   \n",
      "757   -0.839833    0.014409   -0.649910    1.485448  ...    5.320561   \n",
      "758   -0.772052    0.143869   -0.522416    1.547687  ...    5.282432   \n",
      "763   -0.608216   -0.188515    0.618231    0.980057  ...    3.036199   \n",
      "764   -0.977427    1.275853    0.425201    0.907960  ...    1.858904   \n",
      "765   -0.694501    1.018408    0.291793    0.031664  ...    2.761812   \n",
      "\n",
      "     GID6939900  GID6939902  GID6939903  GID6939904  GID6939917  GID6939919  \\\n",
      "0      0.326164    0.337115    0.310114    0.321707   -0.053292    0.547421   \n",
      "1     -0.006584   -0.030811    0.079341   -0.005140    0.987714    0.748617   \n",
      "4     -0.371007   -0.420533   -0.365043   -0.369931   -1.115738   -0.724964   \n",
      "5      0.145743    0.134651    0.166982    0.177469    0.403889    0.973526   \n",
      "6     -1.145479   -1.107706   -1.179701   -1.194947   -1.279294   -1.513275   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "757    5.588681    5.273084    5.310926    5.219214    1.405024    1.952126   \n",
      "758    5.264843    5.600707    5.229067    5.159161    1.509608    2.068934   \n",
      "763    3.069504    3.089260    3.066705    3.026608    1.019147    1.156030   \n",
      "764    1.838179    1.888909    1.757139    1.758256    1.158989    3.113916   \n",
      "765    2.775430    2.686426    2.741873    2.767969    0.809595    1.225058   \n",
      "\n",
      "     GID6939938  GID6939941  GID6939945  \n",
      "0      0.203882   -0.104197    1.052415  \n",
      "1      0.431774    0.709130   -0.346207  \n",
      "4     -0.365307   -0.994924   -1.486580  \n",
      "5      0.105316   -0.020323    0.713237  \n",
      "6     -1.264741   -1.703406   -1.547900  \n",
      "..          ...         ...         ...  \n",
      "757    4.497692    2.760203    4.390804  \n",
      "758    4.522949    2.835786    4.251064  \n",
      "763    7.052105    2.269724    2.417094  \n",
      "764    2.265041    6.827625    1.194760  \n",
      "765    2.274581    1.140682    7.838149  \n",
      "\n",
      "[550 rows x 766 columns]\n",
      "y_train_bin:\n",
      "        GY\n",
      "0    False\n",
      "1     True\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "..     ...\n",
      "757  False\n",
      "758  False\n",
      "763  False\n",
      "764  False\n",
      "765  False\n",
      "\n",
      "[550 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x_params_SVM_B \u001b[38;5;241m=\u001b[39m mdo\u001b[38;5;241m.\u001b[39mAxisParams(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m, bmo\u001b[38;5;241m.\u001b[39mpower_list(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      3\u001b[0m y_params_SVM_B \u001b[38;5;241m=\u001b[39m mdo\u001b[38;5;241m.\u001b[39mAxisParams(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, bmo\u001b[38;5;241m.\u001b[39mpower_list(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m metrics_SVM_B \u001b[38;5;241m=\u001b[39m \u001b[43mouter_CV_B\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_params_SVM_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_params_SVM_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_SVM_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_boundary_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_boundary_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msmote_preprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrbf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m# Dummy values for tests\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mx_params_SVM_B = mdo.AxisParams('gamma', bmo.power_list(2, -10, -9))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03my_params_SVM_B = mdo.AxisParams('C', bmo.power_list(2, 0, 1))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mmetrics_SVM_B = outer_CV_B(2, 2, X, y, x_params_SVM_B, y_params_SVM_B, bmo.train_SVM_classifier, kfold_random_state=RANDOM_STATE, kernel='rbf', classification_col=1, top_boundary_val=top_boundary_val, probability=True)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[54], line 63\u001b[0m, in \u001b[0;36mouter_CV_B\u001b[1;34m(n_outer_splits, n_inner_splits, X, y, axis1_params, axis2_params, train_model_callback, random_state, classification_col, top_boundary_val, smote_preprocess, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Find average best parameters and threshold based on f1 score using inner-fold CV\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m best_param1, best_param2, best_threshold \u001b[38;5;241m=\u001b[39m \u001b[43minner_CV_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_inner_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis1_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis2_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_model_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOuter Fold \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage best parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_param1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_param2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage best threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 44\u001b[0m, in \u001b[0;36minner_CV_B\u001b[1;34m(n_splits, X, y_bin, axis1_params, axis2_params, train_model_callback, kfold_random_state, classification_col, plot_title, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_bin_train)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train numpy 2D array of models with every combination of parameters\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m model_grid \u001b[38;5;241m=\u001b[39m \u001b[43mbmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bin_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis1_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis2_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_model_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Use trained models to predict test set probabilities, and store in 2D array with each cell corresponding to a model with specific combination of parameters\u001b[39;00m\n\u001b[0;32m     47\u001b[0m y_proba_preds_grid \u001b[38;5;241m=\u001b[39m bmo\u001b[38;5;241m.\u001b[39mgrid_predict_proba(X_test, model_grid, classification_col)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\basic_ml_operations.py:209\u001b[0m, in \u001b[0;36mtrain_model_grid\u001b[1;34m(X_train, y_train, axis1_params, axis2_params, train_model_callback, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Loop through each combination of parameters\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (param1, param2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(param_combinations):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Train the model using the train_model_callback and the current parameter combination\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43maxis1_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis2_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam2\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Store the model in the corresponding row and column\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     row_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_cols\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\basic_ml_operations.py:178\u001b[0m, in \u001b[0;36mtrain_SVM_classifier\u001b[1;34m(X_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y_train\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train must be a 1D array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \n\u001b[1;32m--> 178\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:257\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    256\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 257\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\dev\\ml_research\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:335\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    321\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    325\u001b[0m (\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 335\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Real test values\n",
    "x_params_SVM_B = mdo.AxisParams('gamma', bmo.power_list(2, -18, -8))\n",
    "y_params_SVM_B = mdo.AxisParams('C', bmo.power_list(2, 2, 16))\n",
    "metrics_SVM_B = outer_CV_B(5, 10, X, y, x_params_SVM_B, y_params_SVM_B, bmo.train_SVM_classifier, \n",
    "                           random_state=RANDOM_STATE, classification_col=1, top_boundary_val=top_boundary_val, \n",
    "                           smote_preprocess=False, probability=True, kernel='rbf')\n",
    "\n",
    "\"\"\"\n",
    "# Dummy values for tests\n",
    "x_params_SVM_B = mdo.AxisParams('gamma', bmo.power_list(2, -10, -9))\n",
    "y_params_SVM_B = mdo.AxisParams('C', bmo.power_list(2, 0, 1))\n",
    "metrics_SVM_B = outer_CV_B(2, 2, X, y, x_params_SVM_B, y_params_SVM_B, bmo.train_SVM_classifier, kfold_random_state=RANDOM_STATE, kernel='rbf', classification_col=1, top_boundary_val=top_boundary_val, probability=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7219ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification metrics for super-model trained on all data from each outer fold\n",
    "display(metrics_SVM_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_SVM_B_mean = metrics_SVM_B.mean().to_frame().T\n",
    "B_average_metrics.loc['SVM'] = metrics_SVM_B_mean.iloc[0]\n",
    "display(metrics_SVM_B_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf6cd8",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real test values\n",
    "x_params_XGB_B = mdo.AxisParams('n_estimators', [3, 7, 13, 25, 50, 100, 200, 400])\n",
    "y_params_XGB_B = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16, 32, 64])\n",
    "metrics_XGB_B = outer_CV_B(5, 10, X, y, x_params_XGB_B, y_params_XGB_B, bmo.train_XGB_classifier, random_state=RANDOM_STATE, \n",
    "                           random_state=RANDOM_STATE, classification_col=1, top_boundary_val=top_boundary_val, smote_preprocess=SMOGN_PREPROCESS, \n",
    "                           objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "\n",
    "\"\"\"\n",
    "# dummy values for quick tests\n",
    "x_params_XGB_B = mdo.AxisParams('n_estimators', [1, 2])\n",
    "y_params_XGB_B = mdo.AxisParams('max_depth', [1, 2])\n",
    "metrics_XGB_B = outer_CV_B(2, 2, X, y, x_params_XGB_B, y_params_XGB_B, bmo.train_XGB_classifier, kfold_random_state=RANDOM_STATE, random_state=RANDOM_STATE, classification_col=1, top_boundary_val=top_boundary_val, objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "display(metrics_XGB_B)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_XGB_B_mean = metrics_XGB_B.mean().to_frame().T\n",
    "B_average_metrics.loc['XGB'] = metrics_XGB_B_mean.iloc[0]\n",
    "display(metrics_XGB_B_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_B.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de75da",
   "metadata": {},
   "source": [
    "# Model RO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82681b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_CV_RO(n_splits: int, X : pd.DataFrame, y : pd.DataFrame, y_top_bound : float, axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, train_model_callback, \n",
    "                kfold_random_state: int, plot_title: str = \"\", **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform inner cross-validation (RO) to find the best model parameters and classification threshold.\n",
    "    Created: 2024/12/21\n",
    "    Parameters:\n",
    "    n_splits (int): Number of splits for K-Fold cross-validation.\n",
    "    X (pd.DataFrame): Feature data.\n",
    "    y (pd.DataFrame): Target data.\n",
    "    axis1_params (mdo.AxisParams): Hyperparameters to explore for the horizontal axis.\n",
    "    axis2_params (mdo.AxisParams): Hyperparameters to explore for the vertical axis.\n",
    "    train_model_callback (callable): Callback function to train the model.\n",
    "    kfold_random_state (int): Random state for K-Fold shuffling.\n",
    "    top_line_threshold (float): Threshold to classify top values during intermediate step in inner CV.\n",
    "    plot_title (str, optional): Title for the plot. Defaults to \"\".\n",
    "    **kwargs: Additional keyword arguments for the model training callback.\n",
    "    Returns:\n",
    "    tuple: Average best parameters for axis1 and axis2, and the best classification threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create KFold object for inner-fold cross-validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    # Store best parameters (param1, param2) for each fold\n",
    "    best_params = pd.DataFrame(columns=['param1', 'param2'], index=range(n_splits))\n",
    "    best_thresholds = pd.DataFrame(columns=['threshold'], index=range(n_splits))\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        print(f\"    INNER FOLD {i}\")\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train numpy 2D array of models with every combination of parameters\n",
    "        model_grid = bmo.train_model_grid(X_train, y_train, axis1_params, axis2_params, train_model_callback, **kwargs)\n",
    "\n",
    "        # Use trained models to predict test set labels, and store in 2D array with each cell corresponding to a model with specific combination of parameters\n",
    "        y_preds_grid = bmo.grid_predict(X_test, model_grid)\n",
    "\n",
    "        # Create 2D array of identical dataframes containing actual labels to compare against predictions\n",
    "        y_test_grid = cdt.np_array_of_dfs(y_test, y_preds_grid.shape)\n",
    "\n",
    "        # Evaluate predictions by comparing to actuals, calculating 2D array of pearson coefficients\n",
    "        pearson_grid = bmo.calculate_pearson_coefficients(y_preds_grid, y_test_grid)\n",
    "\n",
    "        # Find index of best pearson coefficient in the 2d array of pearson coefficients\n",
    "        best_row, best_col = np.unravel_index(np.argmax(pearson_grid), pearson_grid.shape)\n",
    "        \n",
    "        # Store hyperparameters of most accurate model for this inner fold\n",
    "        best_params.loc[i] = [axis1_params.values[best_row], axis2_params.values[best_col]]\n",
    "        print(f\"    Best parameters found: \\n{best_params.loc[i]}\")\n",
    "\n",
    "        # Extract best model's continuous predictions\n",
    "        best_model_y_preds = y_preds_grid[best_row, best_col]\n",
    "\n",
    "        # Classify labels as top or not top (boolean)\n",
    "        y_test_binary = bmo.continuous_to_binary_absolute(y_test, y_top_bound)\n",
    "        print(f\"    Predictions: \\n{best_model_y_preds}\")\n",
    "        print(f\"    Actuals: \\n{y_test}\")\n",
    "        print(f\"    Top Boundary Value (Calculated from Actuals): {y_top_bound}\")\n",
    "        print(f\"    Binary Actuals: \\n{y_test_binary}\")\n",
    "        \n",
    "        # Find classification threshold that yields lowest squared difference between sensitivity and specificity using this optimal model\n",
    "        best_absolute_thresh = bmo.find_optimal_threshold_absolute(y_test_binary, best_model_y_preds)\n",
    "        best_thresholds.iloc[i, 0] = best_absolute_thresh\n",
    "        print(f\"    Best threshold found: {best_absolute_thresh}\")\n",
    "\n",
    "        # Create grid of scatter plots with predictions vs actuals, coloured by pearson coefficient for each model\n",
    "        scatter_grid = plot_shaded_scatter_grids(y_preds_grid, y_test_grid, axis1_params, axis2_params, pearson_grid, f'{plot_title} | Inner Fold {i}')        \n",
    "        plt.savefig(f'{storage_dir}\\\\model_RO, {train_model_callback.__name__}, ({plot_title}, Inner Fold {i}).svg', format=\"svg\")\n",
    "        plt.show(scatter_grid)\n",
    "        plt.close(scatter_grid)\n",
    "\n",
    "    # Calculate average best parameters over all folds\n",
    "    avg_best_param1 = best_params['param1'].mean()\n",
    "    avg_best_param2 = best_params['param2'].mean()\n",
    "\n",
    "    # Calculate average best threshold over all folds\n",
    "    best_threshold = best_thresholds['threshold'].mean()\n",
    "\n",
    "    return avg_best_param1, avg_best_param2, best_threshold\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_CV_RO(n_outer_splits: int, n_inner_splits: int, X : pd.DataFrame, y : pd.DataFrame, \n",
    "                axis1_params: mdo.AxisParams, axis2_params: mdo.AxisParams, train_model_callback : callable, \n",
    "                kfold_random_state: int, top_boundary_val : float, smogn_preprocess : bool = False, undersample : bool = True,\n",
    "                **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform outer cross-validation with nested inner cross-validation for model RO selection and evaluation.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_outer_splits : int\n",
    "        Number of splits for the outer cross-validation.\n",
    "    n_inner_splits : int\n",
    "        Number of splits for the inner cross-validation.\n",
    "    X : pd.DataFrame\n",
    "        Feature data.\n",
    "    y : pd.DataFrame\n",
    "        Target data.\n",
    "    axis1_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the first axis.\n",
    "    axis2_params : mdo.AxisParams\n",
    "        Object representing hyperparameter search space for the second axis.\n",
    "    train_model_callback : callable\n",
    "        Callback function to train the model.\n",
    "    kfold_random_state : int\n",
    "        Random state for reproducibility in KFold splitting.\n",
    "    top_line_threshold : float\n",
    "        Threshold for classifying top predictions during intermediate step in inner CV.\n",
    "    **kwargs : dict\n",
    "        Additional parameters for the model training callback.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the evaluation metrics for each outer fold, including Pearson correlation, F1 Score, Sensitivity, Specificity, and Kappa.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create KFold object for outer loop to split data into train and test sets\n",
    "    kfold = KFold(n_splits=n_outer_splits, shuffle=True, random_state=kfold_random_state)\n",
    "\n",
    "    kfold_metrics = pd.DataFrame(columns=['Pearson', 'F1 Score', 'Sensitivity', 'Specificity', 'Kappa'])\n",
    "\n",
    "    # Iterate through each train-test split\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        print(f\"OUTER CV: {train_model_callback.__name__}, FOLD {i}\")\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if smogn_preprocess:\n",
    "            top_boundary_quantile_in_train_set = percentileofscore(y_train.to_numpy().flatten(), top_boundary_val, kind='mean') / 100\n",
    "\n",
    "            X_train_pre_smogn = X_train.copy()\n",
    "            y_train_pre_smogn = y_train.copy()\n",
    "            X_train, y_train = smogn_prep(X_train, y_train, top_boundary_quantile_in_train_set, undersample)\n",
    "\n",
    "            if not undersample:\n",
    "                # Unfortunately the SMOGN library for some reason only returns augmented data when undersampling is disabled. \n",
    "                # This means we need to manually concatenate the original data below the augmentation threshold with the \n",
    "                # augmented data. Read more: https://github.com/nickkunz/smogn/issues/21\n",
    "                non_augmented_indices = y_train_pre_smogn[y_train_pre_smogn < top_boundary_val].index\n",
    "                X_train_non_augmented = X_train_pre_smogn.loc[non_augmented_indices]\n",
    "                y_train_non_augmented = y_train_pre_smogn.loc[non_augmented_indices]\n",
    "                X_train = pd.concat([X_train, X_train_non_augmented], axis=0)\n",
    "                y_train = pd.concat([y_train, y_train_non_augmented], axis=0)\n",
    "            plot_GY_hist(y_train, f'Model RO SMOGN-Augmented GY Histogram, Outer Fold {i}')\n",
    "\n",
    "        X_train, y_train, X_scaler, y_scaler = scale_features_and_target(X_train, y_train)\n",
    "        top_boundary_val_scaled = y_scaler.transform([[top_boundary_val]])[0, 0]\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "        y_test = y_scaler.transform(y_test)\n",
    "\n",
    "        # Find average best parameters and threshold based on pearson score using inner-fold CV\n",
    "        best_param1, best_param2, best_threshold_fixed = inner_CV_RO(n_inner_splits, X_train, y_train, top_boundary_val_scaled, axis1_params, axis2_params, train_model_callback, kfold_random_state, plot_title=f\"Outer Fold {i}\", **kwargs)\n",
    "        print(f\"Average best parameters: {best_param1}, {best_param2}\")\n",
    "        print(f\"Average best threshold: {best_threshold_fixed}\")\n",
    "\n",
    "        # Train model with all training and CV data of outer fold using mean best hyperparameters\n",
    "        super_model = train_model_callback(X_train, np.ravel(y_train), **dict(zip([axis1_params.name, axis2_params.name], [best_param1, best_param2])), **kwargs)\n",
    "\n",
    "        # Use trained \"super-model\" to predict test set\n",
    "        y_pred = pd.DataFrame(super_model.predict(X_test), index=y_test.index, columns=y_test.columns)\n",
    "        plot_GY_hist(y_pred, f'Predicted GY Histogram, {train_model_callback.__name__}, Outer Fold {i}')\n",
    "        print(f\"Super model predictions: \\n{y_pred}\")\n",
    "\n",
    "        # Calculate pearson coefficient of continuous predictions\n",
    "        pearson, _ = pearsonr(np.ravel(y_pred), np.ravel(y_test))\n",
    "\n",
    "        # Classify predictions and actuals of super_model as top or not top (boolean)\n",
    "        y_pred_top = bmo.continuous_to_binary_absolute(y_pred, best_threshold_fixed)\n",
    "        y_test_top = bmo.continuous_to_binary_absolute(y_test, top_boundary_val_scaled)\n",
    "\n",
    "        print(y_test_top)\n",
    "\n",
    "        cmp.plot_classification_results(y_pred, y_test, y_pred_top, y_test_top, \n",
    "                                [f\"Model RO Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}\"],\n",
    "                                save_path=f'{storage_dir}\\\\Model RO Super Model Predicted vs Actual GY, {train_model_callback.__name__}, Outer Fold {i}.svg')\n",
    "\n",
    "        # Calculate classification metrics and add new row to kfold_metrics\n",
    "        classification_metrics = cdt.classification_metrics(y_pred_top, y_test_top)\n",
    "        pearson_df = pd.DataFrame([pearson], columns=['Pearson'])\n",
    "        metrics_row = pd.concat([pearson_df, classification_metrics], axis=1)\n",
    "        kfold_metrics = pd.concat([kfold_metrics, metrics_row], axis=0)\n",
    "    \n",
    "    kfold_metrics.index = range(n_outer_splits)\n",
    "    return kfold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac635306",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Real values\n",
    "x_params_SVM_RO = mdo.AxisParams('gamma', bmo.power_list(2, -14, -6))\n",
    "y_params_SVM_RO = mdo.AxisParams('C', bmo.power_list(2, -2, 6))\n",
    "metrics_SVM_RO = outer_CV_RO(5, 10, X, y, x_params_SVM_RO, y_params_SVM_RO, bmo.train_SVM_regressor, \n",
    "                             kfold_random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, smogn_preprocess=SMOGN_PREPROCESS, \n",
    "                             undersample=UNDERSAMPLE, kernel='rbf')\n",
    "\n",
    "\"\"\"# Quick test values\n",
    "x_params_SVM_RO = mdo.AxisParams('gamma', bmo.power_list(2, -8, -7))\n",
    "y_params_SVM_RO = mdo.AxisParams('C', bmo.power_list(2, 0, 1))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975314ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "display(metrics_SVM_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb307e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_SVM_RO_mean = metrics_SVM_RO.mean().to_frame().T\n",
    "RO_average_metrics.loc['SVM'] = metrics_SVM_RO_mean.iloc[0]\n",
    "display(metrics_SVM_RO_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2313db",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_params_XGB_RO = mdo.AxisParams('n_estimators', [3, 7, 13, 25, 50, 100, 200])\n",
    "y_params_XGB_RO = mdo.AxisParams('max_depth', [1, 2, 3, 4, 6, 10, 16, 32, 64])\n",
    "metrics_XGB_RO = outer_CV_RO(5, 10, X, y, x_params_XGB_RO, y_params_XGB_RO, bmo.train_XGB_regressor, \n",
    "                             kfold_random_state=RANDOM_STATE, random_state=RANDOM_STATE, top_boundary_val=top_boundary_val, \n",
    "                             smogn_preprocess=SMOGN_PREPROCESS, undersample=UNDERSAMPLE, objective=\"reg:squarederror\", \n",
    "                             eval_metric=\"rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37545449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "display(metrics_XGB_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521925c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average of each metric\n",
    "metrics_XGB_RO_mean = metrics_XGB_RO.mean().to_frame().T\n",
    "RO_average_metrics.loc['XGB'] = metrics_XGB_RO_mean.iloc[0]\n",
    "display(metrics_XGB_RO_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized session variables and models to disk for later use\n",
    "dill.dump_session(f'{storage_dir}\\\\project_ipynb_env_RO.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdc2c5",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average metrics for each model to compare GBLUP, SVM, and XGB\n",
    "R_avg_metrics_plot = cmp.plot_model_metrics(R_average_metrics, \"R\")\n",
    "R_avg_metrics_plot.savefig(f'{storage_dir}\\\\R_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(R_avg_metrics_plot)\n",
    "plt.close(R_avg_metrics_plot)\n",
    "\n",
    "B_avg_metrics_plot = cmp.plot_model_metrics(B_average_metrics, \"B\")\n",
    "B_avg_metrics_plot.savefig(f'{storage_dir}\\\\B_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(B_avg_metrics_plot)\n",
    "plt.close(B_avg_metrics_plot)\n",
    "\n",
    "RO_avg_metrics_plot = cmp.plot_model_metrics(RO_average_metrics, \"RO\")\n",
    "RO_avg_metrics_plot.savefig(f'{storage_dir}\\\\RO_avg_metrics_plot.svg', format='svg')\n",
    "plt.show(RO_avg_metrics_plot)\n",
    "plt.close(RO_avg_metrics_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62323388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
